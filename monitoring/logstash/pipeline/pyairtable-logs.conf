# Logstash Pipeline Configuration for PyAirtable Platform
# Comprehensive log processing with cost-aware parsing and enrichment

input {
  # Beats input (Filebeat)
  beats {
    port => 5044
  }
  
  # Syslog input (optional)
  syslog {
    port => 5000
    type => "syslog"
  }
  
  # HTTP input for direct log shipping
  http {
    port => 8080
    type => "http"
    codec => json
  }
}

filter {
  # Add pipeline metadata
  mutate {
    add_field => {
      "[@metadata][pipeline]" => "pyairtable-logs"
      "[@metadata][version]" => "1.0.0"
    }
  }
  
  # Parse container logs
  if [container][name] {
    # Clean up container name
    mutate {
      gsub => [ "[container][name]", "^/", "" ]
      gsub => [ "[container][name]", "-[0-9]+$", "" ]
    }
    
    # Set service name from container
    if ![service][name] {
      mutate {
        add_field => { "[service][name]" => "%{[container][name]}" }
      }
    }
  }
  
  # Extract correlation ID from various sources
  if [fields][correlation_id] {
    mutate {
      add_field => { "[correlation][id]" => "%{[fields][correlation_id]}" }
    }
  } else if [correlation_id] {
    mutate {
      add_field => { "[correlation][id]" => "%{correlation_id}" }
    }
  } else if [request_id] {
    mutate {
      add_field => { "[correlation][id]" => "%{request_id}" }
    }
  } else if [x-correlation-id] {
    mutate {
      add_field => { "[correlation][id]" => "%{x-correlation-id}" }
    }
  }
  
  # Extract trace context
  if [trace_id] {
    mutate {
      add_field => { "[trace][id]" => "%{trace_id}" }
    }
  } else if [fields][trace_id] {
    mutate {
      add_field => { "[trace][id]" => "%{[fields][trace_id]}" }
    }
  }
  
  if [span_id] {
    mutate {
      add_field => { "[trace][span_id]" => "%{span_id}" }
    }
  } else if [fields][span_id] {
    mutate {
      add_field => { "[trace][span_id]" => "%{[fields][span_id]}" }
    }
  }
  
  # Extract user and tenant context
  if [user_id] {
    mutate {
      add_field => { "[user][id]" => "%{user_id}" }
    }
  } else if [fields][user_id] {
    mutate {
      add_field => { "[user][id]" => "%{[fields][user_id]}" }
    }
  }
  
  if [tenant_id] {
    mutate {
      add_field => { "[tenant][id]" => "%{tenant_id}" }
    }
  } else if [fields][tenant_id] {
    mutate {
      add_field => { "[tenant][id]" => "%{[fields][tenant_id]}" }
    }
  }
  
  # Service-specific parsing
  if [service][name] {
    
    # API Gateway logs (Go/Fiber)
    if [service][name] =~ /api-gateway/ {
      grok {
        match => { 
          "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{LOGLEVEL:log_level}\] %{GREEDYDATA:log_message}"
        }
        add_field => { 
          "[service][tier]" => "gateway"
          "[cost][center]" => "gateway"
        }
      }
      
      # Parse HTTP access logs
      if [log_message] =~ /\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}/ {
        grok {
          match => {
            "log_message" => "%{IP:client_ip} - - \[%{HTTPDATE:access_time}\] \"%{WORD:http_method} %{URIPATHPARAM:request_path} HTTP/%{NUMBER:http_version}\" %{NUMBER:response_code} %{NUMBER:response_size} \"%{DATA:referrer}\" \"%{DATA:user_agent}\" %{NUMBER:response_time_ms}ms"
          }
          add_field => {
            "[log][type]" => "access"
            "[http][method]" => "%{http_method}"
            "[http][response_code]" => "%{response_code}"
            "[http][response_time_ms]" => "%{response_time_ms}"
          }
        }
        
        # Categorize response time performance
        if [response_time_ms] {
          ruby {
            code => "
              response_time = event.get('response_time_ms').to_f
              if response_time < 100
                event.set('[performance][bucket]', 'fast')
              elsif response_time < 500
                event.set('[performance][bucket]', 'normal')
              elsif response_time < 2000
                event.set('[performance][bucket]', 'slow')
              elsif response_time < 10000
                event.set('[performance][bucket]', 'very_slow')
              else
                event.set('[performance][bucket]', 'timeout_risk')
              end
            "
          }
        }
      }
    }
    
    # LLM Orchestrator logs (Python/FastAPI)
    else if [service][name] =~ /llm-orchestrator/ {
      # Parse structured JSON logs
      if [message] =~ /^\{/ {
        json {
          source => "message"
          target => "parsed"
        }
        
        if [parsed] {
          mutate {
            add_field => {
              "[log][level]" => "%{[parsed][level]}"
              "[log][message]" => "%{[parsed][message]}"
              "[service][tier]" => "ai-ml"
              "[cost][center]" => "ai-compute"
            }
          }
          
          # Extract AI-specific metrics
          if [parsed][ai] {
            mutate {
              add_field => {
                "[ai][provider]" => "%{[parsed][ai][provider]}"
                "[ai][model]" => "%{[parsed][ai][model]}"
                "[ai][input_tokens]" => "%{[parsed][ai][input_tokens]}"
                "[ai][output_tokens]" => "%{[parsed][ai][output_tokens]}"
                "[ai][cost_usd]" => "%{[parsed][ai][cost_usd]}"
              }
            }
          }
          
          # Extract trace information
          if [parsed][trace_id] {
            mutate {
              add_field => {
                "[trace][id]" => "%{[parsed][trace_id]}"
                "[span][id]" => "%{[parsed][span_id]}"
              }
            }
          }
        }
      }
      # Parse standard Python logs
      else {
        grok {
          match => {
            "message" => "%{TIMESTAMP_ISO8601:timestamp} - %{DATA:logger_name} - %{LOGLEVEL:log_level} - %{GREEDYDATA:log_message}"
          }
          add_field => {
            "[service][tier]" => "ai-ml"
            "[cost][center]" => "ai-compute"
          }
        }
      }
    }
    
    # Platform Services logs (Python/FastAPI)
    else if [service][name] =~ /platform-services/ {
      json {
        source => "message"
        target => "parsed"
      }
      
      mutate {
        add_field => {
          "[service][tier]" => "platform"
          "[cost][center]" => "platform"
        }
      }
      
      # Parse authentication events
      if [log_message] =~ /auth|login|register|token/ {
        mutate {
          add_field => { "[event][category]" => "authentication" }
        }
      }
      
      # Parse analytics events
      if [log_message] =~ /analytics|metrics|tracking/ {
        mutate {
          add_field => { "[event][category]" => "analytics" }
        }
      }
    }
    
    # Automation Services logs
    else if [service][name] =~ /automation-services/ {
      json {
        source => "message"
        target => "parsed"
      }
      
      mutate {
        add_field => {
          "[service][tier]" => "automation"
          "[cost][center]" => "automation"
        }
      }
      
      # Parse workflow events
      if [parsed][workflow] {
        mutate {
          add_field => {
            "[workflow][id]" => "%{[parsed][workflow][id]}"
            "[workflow][type]" => "%{[parsed][workflow][type]}"
            "[workflow][status]" => "%{[parsed][workflow][status]}"
          }
        }
      }
    }
    
    # Database logs (PostgreSQL)
    else if [service][name] =~ /postgres/ {
      grok {
        match => {
          "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{NUMBER:pid}\] %{LOGLEVEL:log_level}:  %{GREEDYDATA:log_message}"
        }
        add_field => {
          "[service][tier]" => "database"
          "[cost][center]" => "infrastructure"
        }
      }
      
      # Parse slow query logs
      if [log_message] =~ /duration:|slow query/ {
        grok {
          match => {
            "log_message" => "duration: %{NUMBER:query_duration_ms} ms.*statement: %{GREEDYDATA:sql_statement}"
          }
          add_field => {
            "[database][query_type]" => "slow"
            "[performance][query_duration_ms]" => "%{query_duration_ms}"
          }
        }
      }
    }
    
    # Redis logs
    else if [service][name] =~ /redis/ {
      grok {
        match => {
          "message" => "%{NUMBER:pid}:%{CHAR:role} %{TIMESTAMP_ISO8601:timestamp} %{CHAR:log_level} %{GREEDYDATA:log_message}"
        }
        add_field => {
          "[service][tier]" => "cache"
          "[cost][center]" => "infrastructure"
        }
      }
    }
    
    # Observability stack logs
    else if [service][name] =~ /(prometheus|grafana|jaeger|elasticsearch|kibana|logstash)/ {
      mutate {
        add_field => {
          "[service][tier]" => "observability"
          "[cost][center]" => "observability"
        }
      }
    }
  }
  
  # Parse log levels consistently
  if [log_level] {
    mutate {
      uppercase => [ "log_level" ]
    }
  }
  
  # Add GeoIP information for client IPs
  if [client_ip] and [client_ip] !~ /^(10\.|172\.(1[6-9]|2[0-9]|3[01])\.|192\.168\.|127\.)/ {
    geoip {
      source => "client_ip"
      target => "client_geo"
    }
  }
  
  # Extract and parse user agent
  if [user_agent] {
    useragent {
      source => "user_agent"
      target => "ua"
    }
  }
  
  # Parse and validate timestamp
  if [timestamp] {
    date {
      match => [ "timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss,SSS", "yyyy-MM-dd HH:mm:ss.SSS" ]
      target => "@timestamp"
    }
  }
  
  # Add cost tracking fields
  ruby {
    code => "
      service_tier = event.get('[service][tier]')
      log_level = event.get('[log_level]')
      
      # Calculate cost weight based on service tier and log level
      cost_weight = 1
      case service_tier
      when 'ai-ml'
        cost_weight = 5  # AI/ML services have higher processing cost
      when 'gateway'
        cost_weight = 3  # Gateway handles all traffic
      when 'platform'
        cost_weight = 2  # Core platform services
      else
        cost_weight = 1  # Default weight
      end
      
      # Adjust for log level (errors cost more to process/investigate)
      case log_level
      when 'ERROR'
        cost_weight *= 3
      when 'WARN'
        cost_weight *= 2
      when 'INFO'
        cost_weight *= 1
      when 'DEBUG'
        cost_weight *= 0.5
      end
      
      event.set('[cost][weight]', cost_weight)
      event.set('[cost][estimated_processing_cost_cents]', cost_weight * 0.01)
    "
  }
  
  # Security event detection
  if [log_message] =~ /(failed login|unauthorized|403|401|security|attack|injection|xss)/ {
    mutate {
      add_field => {
        "[security][event]" => "true"
        "[alert][priority]" => "high"
      }
      add_tag => [ "security_event" ]
    }
  }
  
  # Error event detection
  if [log_level] == "ERROR" or [response_code] >= 500 {
    mutate {
      add_field => {
        "[error][event]" => "true"
        "[alert][priority]" => "medium"
      }
      add_tag => [ "error_event" ]
    }
  }
  
  # Performance event detection
  if [response_time_ms] and [response_time_ms] > 2000 {
    mutate {
      add_field => {
        "[performance][slow_request]" => "true"
        "[alert][priority]" => "low"
      }
      add_tag => [ "performance_event" ]
    }
  }
  
  # Clean up intermediate fields
  mutate {
    remove_field => [ "parsed", "host", "agent", "@version", "input", "ecs" ]
  }
}

output {
  # Main Elasticsearch output
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    
    # Use date-based indices for better management
    index => "pyairtable-logs-%{+YYYY.MM.dd}"
    
    # Document type
    document_type => "_doc"
    
    # Template configuration
    template_name => "pyairtable-logs"
    template => "/usr/share/logstash/templates/pyairtable-logs.json"
    template_overwrite => true
    
    # ILM configuration
    ilm_enabled => true
    ilm_rollover_alias => "pyairtable-logs"
    ilm_pattern => "{now/d}-000001"
    ilm_policy => "pyairtable-logs-policy"
  }
  
  # Separate index for security events
  if "security_event" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "pyairtable-security-%{+YYYY.MM.dd}"
      template_name => "pyairtable-security"
    }
  }
  
  # Separate index for error events
  if "error_event" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "pyairtable-errors-%{+YYYY.MM.dd}"
      template_name => "pyairtable-errors"
    }
  }
  
  # Output to file for debugging (development only)
  if [@metadata][environment] == "development" {
    file {
      path => "/tmp/logstash-debug-%{+YYYY.MM.dd}.log"
      codec => json_lines
    }
  }
  
  # Metrics output for monitoring pipeline performance
  statsd {
    host => "localhost"
    port => 8125
    gauge => {
      "logstash.events.processed" => 1
      "logstash.events.%{[service][name]}" => 1
      "logstash.events.%{[service][tier]}" => 1
    }
  }
}
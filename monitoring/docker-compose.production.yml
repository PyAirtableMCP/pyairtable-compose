version: '3.8'

# Production LGTM Stack for PyAirtable
# Optimized for production workloads with enhanced resource limits,
# data retention policies, and high availability configurations

services:
  # MinIO for production object storage
  minio:
    image: minio/minio:RELEASE.2024-01-16T16-07-38Z
    container_name: minio-prod
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-pyairtable_admin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-PyAirtable2025!}
      MINIO_PROMETHEUS_AUTH_TYPE: public
      MINIO_PROMETHEUS_URL: http://mimir:8080
      MINIO_BROWSER_REDIRECT_URL: https://minio.pyairtable.com
    command: server /data --console-address ":9001"
    volumes:
      - minio-prod-data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
      start_period: 60s
    networks:
      - monitoring
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # MinIO client for bucket initialization with production buckets
  minio-init:
    image: minio/mc:RELEASE.2024-01-13T08-44-48Z
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      sleep 10;
      /usr/bin/mc alias set minio http://minio:9000 ${MINIO_ROOT_USER:-pyairtable_admin} ${MINIO_ROOT_PASSWORD:-PyAirtable2025!};
      /usr/bin/mc mb minio/loki-data || true;
      /usr/bin/mc mb minio/loki-ruler || true;
      /usr/bin/mc mb minio/tempo-data || true;
      /usr/bin/mc mb minio/mimir-blocks || true;
      /usr/bin/mc mb minio/mimir-ruler || true;
      /usr/bin/mc mb minio/mimir-alertmanager || true;
      /usr/bin/mc mb minio/mimir-compactor || true;
      /usr/bin/mc policy set public minio/loki-data;
      /usr/bin/mc policy set public minio/loki-ruler;
      /usr/bin/mc policy set public minio/tempo-data;
      /usr/bin/mc policy set public minio/mimir-blocks;
      /usr/bin/mc policy set public minio/mimir-ruler;
      /usr/bin/mc policy set public minio/mimir-alertmanager;
      /usr/bin/mc policy set public minio/mimir-compactor;
      echo 'MinIO buckets initialized successfully';
      exit 0;
      "
    networks:
      - monitoring

  # Loki - Production log aggregation with retention policies
  loki:
    image: grafana/loki:2.9.4
    container_name: loki-prod
    ports:
      - "3100:3100"
      - "7946:7946"  # memberlist
    command: -config.file=/etc/loki/loki.yml
    volumes:
      - ./lgtm-stack/loki/loki-production.yml:/etc/loki/loki.yml:ro
      - loki-prod-data:/loki
    depends_on:
      minio-init:
        condition: service_completed_successfully
    restart: unless-stopped
    networks:
      - monitoring
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # Tempo - Production distributed tracing
  tempo:
    image: grafana/tempo:2.3.1
    container_name: tempo-prod
    ports:
      - "3200:3200"   # HTTP
      - "14268:14268" # Jaeger HTTP
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
      - "9095:9095"   # gRPC internal
    command: -config.file=/etc/tempo/tempo.yml
    volumes:
      - ./lgtm-stack/tempo/tempo-production.yml:/etc/tempo/tempo.yml:ro
      - tempo-prod-data:/var/tempo
    depends_on:
      minio-init:
        condition: service_completed_successfully
    restart: unless-stopped
    networks:
      - monitoring
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3200/ready || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # Mimir - Production long-term metrics storage
  mimir:
    image: grafana/mimir:2.10.3
    container_name: mimir-prod
    ports:
      - "8080:8080"   # HTTP
      - "7946:7946"   # memberlist
    command: -config.file=/etc/mimir/mimir.yml
    volumes:
      - ./lgtm-stack/mimir/mimir-production.yml:/etc/mimir/mimir.yml:ro
      - mimir-prod-data:/data
    depends_on:
      minio-init:
        condition: service_completed_successfully
    restart: unless-stopped
    networks:
      - monitoring
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4.0'
        reservations:
          memory: 4G
          cpus: '2.0'
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:8080/ready || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # Grafana - Production visualization with enhanced security
  grafana:
    image: grafana/grafana:10.2.0
    container_name: grafana-prod
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-PyAirtable2025!}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_USERS_AUTO_ASSIGN_ORG_ROLE=Viewer
      - GF_SECURITY_SECRET_KEY=${GRAFANA_SECRET_KEY:-PyAirtableSecretKey2025}
      - GF_SECURITY_COOKIE_SECURE=true
      - GF_SECURITY_COOKIE_SAMESITE=strict
      - GF_SERVER_ROOT_URL=https://grafana.pyairtable.com
      - GF_ANALYTICS_REPORTING_ENABLED=false
      - GF_ANALYTICS_CHECK_FOR_UPDATES=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel,grafana-worldmap-panel,grafana-clock-panel,grafana-polystat-panel,vonage-status-panel
      - GF_FEATURE_TOGGLES_ENABLE=traceqlEditor,correlations
      - GF_ALERTING_ENABLED=true
      - GF_UNIFIED_ALERTING_ENABLED=true
      - GF_ALERTING_EXECUTE_ALERTS=true
      - GF_DATABASE_TYPE=sqlite3
      - GF_DATABASE_PATH=/var/lib/grafana/grafana.db
      - GF_LOG_LEVEL=warn
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - ./grafana/dashboards:/var/lib/grafana/dashboards:ro
      - grafana-prod-data:/var/lib/grafana
    depends_on:
      loki:
        condition: service_healthy
      tempo:
        condition: service_healthy
      mimir:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - monitoring
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # Promtail - Production log collection with filtering
  promtail:
    image: grafana/promtail:2.9.4
    container_name: promtail-prod
    volumes:
      - ./lgtm-stack/promtail/promtail-production.yml:/etc/promtail/config.yml:ro
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - promtail-positions:/tmp/positions
    command: -config.file=/etc/promtail/config.yml
    depends_on:
      loki:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - monitoring
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # OpenTelemetry Collector - Production telemetry processing
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.89.0
    container_name: otel-collector-prod
    ports:
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
      - "8889:8889"   # Prometheus metrics
      - "13133:13133" # Health check
      - "14250:14250" # Jaeger gRPC
      - "55679:55679" # zpages
    command: ["--config=/etc/otelcol-contrib/otel-collector.yml"]
    volumes:
      - ./lgtm-stack/otel/otel-collector-production.yml:/etc/otelcol-contrib/otel-collector.yml:ro
    depends_on:
      tempo:
        condition: service_healthy
      mimir:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - monitoring
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:13133 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # AlertManager - Production alerting with redundancy
  alertmanager:
    image: prom/alertmanager:v0.26.0
    container_name: alertmanager-prod
    ports:
      - "9093:9093"
      - "9094:9094"  # cluster
    volumes:
      - ./alertmanager/alertmanager-production.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager-prod-data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=https://alertmanager.pyairtable.com'
      - '--cluster.listen-address=0.0.0.0:9094'
      - '--cluster.peer=alertmanager-backup:9094'
      - '--cluster.reconnect-timeout=10m'
      - '--cluster.gossip-interval=200ms'
      - '--cluster.pushpull-interval=60s'
      - '--web.route-prefix=/'
      - '--log.level=info'
    restart: unless-stopped
    networks:
      - monitoring
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9093/-/healthy || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # Backup AlertManager for high availability
  alertmanager-backup:
    image: prom/alertmanager:v0.26.0
    container_name: alertmanager-backup-prod
    ports:
      - "9095:9093"
      - "9096:9094"  # cluster
    volumes:
      - ./alertmanager/alertmanager-production.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager-backup-data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=https://alertmanager-backup.pyairtable.com'
      - '--cluster.listen-address=0.0.0.0:9094'
      - '--cluster.peer=alertmanager:9094'
      - '--cluster.reconnect-timeout=10m'
      - '--cluster.gossip-interval=200ms'
      - '--cluster.pushpull-interval=60s'
      - '--web.route-prefix=/'
      - '--log.level=info'
    restart: unless-stopped
    networks:
      - monitoring
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9093/-/healthy || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # Node Exporter for system metrics
  node-exporter:
    image: prom/node-exporter:v1.6.1
    container_name: node-exporter-prod
    ports:
      - "9100:9100"
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--path.rootfs=/rootfs'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
      - '--collector.netdev.device-exclude=^(veth.*|docker.*|br-.*|.*-br)$$'
      - '--collector.textfile.directory=/var/lib/node-exporter/textfile'
      - '--web.listen-address=:9100'
      - '--log.level=info'
    volumes:
      - '/proc:/host/proc:ro'
      - '/sys:/host/sys:ro'
      - '/:/rootfs:ro'
      - 'node-exporter-textfile:/var/lib/node-exporter/textfile'
    restart: unless-stopped
    networks:
      - monitoring
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
        reservations:
          memory: 128M
          cpus: '0.1'
    pid: host
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # cAdvisor for container metrics
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.47.0
    container_name: cadvisor-prod
    ports:
      - "8080:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /var/lib/docker:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    restart: unless-stopped
    networks:
      - monitoring
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    privileged: true
    devices:
      - /dev/kmsg:/dev/kmsg
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

volumes:
  minio-prod-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/pyairtable/monitoring/data/minio
  loki-prod-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/pyairtable/monitoring/data/loki
  tempo-prod-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/pyairtable/monitoring/data/tempo
  mimir-prod-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/pyairtable/monitoring/data/mimir
  grafana-prod-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/pyairtable/monitoring/data/grafana
  alertmanager-prod-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/pyairtable/monitoring/data/alertmanager
  alertmanager-backup-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/pyairtable/monitoring/data/alertmanager-backup
  promtail-positions:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/pyairtable/monitoring/data/promtail
  node-exporter-textfile:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/pyairtable/monitoring/data/node-exporter

networks:
  monitoring:
    driver: bridge
    name: pyairtable-monitoring
    ipam:
      config:
        - subnet: 172.21.0.0/16
          gateway: 172.21.0.1
    driver_opts:
      com.docker.network.bridge.name: pyairtable-mon0
      com.docker.network.driver.mtu: 1500
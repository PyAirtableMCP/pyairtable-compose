# OpenTelemetry Collector Production Configuration
# Enhanced telemetry processing with LGTM stack integration,
# advanced sampling, and business metrics extraction

receivers:
  # OTLP receivers for traces, metrics, and logs
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
        max_recv_msg_size_mib: 32
        max_concurrent_streams: 16
        read_buffer_size: 512KiB
        write_buffer_size: 512KiB
      http:
        endpoint: 0.0.0.0:4318
        max_request_body_size: 32MiB

  # Jaeger receivers for legacy support
  jaeger:
    protocols:
      grpc:
        endpoint: 0.0.0.0:14250
      thrift_http:
        endpoint: 0.0.0.0:14268
      thrift_compact:
        endpoint: 0.0.0.0:6831
      thrift_binary:
        endpoint: 0.0.0.0:6832

  # Zipkin receiver
  zipkin:
    endpoint: 0.0.0.0:9411

  # Prometheus receiver for scraping metrics
  prometheus:
    config:
      global:
        scrape_interval: 15s
        evaluation_interval: 15s
        external_labels:
          environment: production
          cluster: pyairtable
      scrape_configs:
        # Scrape PyAirtable services
        - job_name: 'pyairtable-services'
          static_configs:
            - targets:
                - 'pyairtable-gateway:8080'
                - 'pyairtable-ai:8080'
                - 'pyairtable-airtable:8080'
                - 'pyairtable-automation:8080'
                - 'pyairtable-platform:8080'
          scrape_interval: 15s
          metrics_path: /metrics
          
        # Scrape infrastructure
        - job_name: 'infrastructure'
          static_configs:
            - targets:
                - 'node-exporter:9100'
                - 'cadvisor:8080'
          scrape_interval: 30s
          
        # Scrape LGTM stack
        - job_name: 'lgtm-stack'
          static_configs:
            - targets:
                - 'loki:3100'
                - 'tempo:3200'
                - 'mimir:8080'
                - 'grafana:3000'
          scrape_interval: 30s

  # Host metrics receiver
  hostmetrics:
    collection_interval: 30s
    scrapers:
      cpu:
        metrics:
          system.cpu.utilization:
            enabled: true
      disk:
        metrics:
          system.disk.utilization:
            enabled: true
      load:
      memory:
        metrics:
          system.memory.utilization:
            enabled: true
      network:
      paging:
        metrics:
          system.paging.utilization:
            enabled: true
      processes:
      filesystem:
        metrics:
          system.filesystem.utilization:
            enabled: true

  # Docker stats receiver
  docker_stats:
    endpoint: unix:///var/run/docker.sock
    collection_interval: 30s
    timeout: 10s

processors:
  # Memory limiter to prevent OOM
  memory_limiter:
    limit_mib: 1024
    spike_limit_mib: 256
    check_interval: 5s

  # Batch processor for efficiency
  batch:
    timeout: 1s
    send_batch_size: 1024
    send_batch_max_size: 2048

  # Resource processor to add standard attributes
  resource:
    attributes:
      - key: environment
        value: production
        action: upsert
      - key: cluster
        value: pyairtable
        action: upsert
      - key: collector.name
        value: otel-collector-prod
        action: upsert
      - key: collector.version
        value: 0.89.0
        action: upsert

  # Attributes processor for trace enhancement
  attributes:
    actions:
      # Add business context to spans
      - key: business.tenant_id
        from_attribute: tenant_id
        action: upsert
      - key: business.user_id
        from_attribute: user_id
        action: upsert
      - key: business.cost_center
        from_attribute: cost_center
        action: upsert
      
  # Span processor for sampling and filtering
  span:
    name:
      # Rename spans for consistency
      from_attributes: [http.method, http.route]
      separator: " "
    # Drop noisy spans
    include:
      match_type: regexp
      services: ["pyairtable-.*"]
    exclude:
      match_type: strict
      span_names: ["health-check", "metrics"]

  # Probabilistic sampler for traces
  probabilistic_sampler:
    hash_seed: 22
    sampling_percentage: 1.0  # 1% sampling for production

  # Tail sampling for intelligent trace selection
  tail_sampling:
    decision_wait: 10s
    num_traces: 50000
    expected_new_traces_per_sec: 10
    policies:
      # Always sample errors
      - name: errors
        type: status_code
        status_code: {status_codes: [ERROR]}
      
      # Always sample slow requests
      - name: slow-requests
        type: latency
        latency: {threshold_ms: 1000}
      
      # Sample based on service
      - name: critical-services
        type: string_attribute
        string_attribute: 
          key: service.name
          values: ["pyairtable-gateway", "pyairtable-ai"]
          enabled_regex_matching: true
          invert_match: false
        
      # Probabilistic sampling for everything else
      - name: probabilistic
        type: probabilistic
        probabilistic: {sampling_percentage: 0.1}

  # Transform processor for metrics
  transform:
    metric_statements:
      # Convert histogram to summary for better performance
      - context: metric
        statements:
          - set(description, "Request duration in seconds") where name == "http_request_duration_seconds"
          - set(unit, "s") where name == "http_request_duration_seconds"
      
      # Add business metrics context
      - context: datapoint
        statements:
          - set(attributes["business.domain"], "api") where attributes["service.name"] != nil
          - set(attributes["cost.unit"], "request") where metric.name == "http_requests_total"

  # Filter processor for metrics
  filter:
    metrics:
      # Exclude high-cardinality metrics
      exclude:
        match_type: regexp
        metric_names:
          - '.*_bucket'
          - '.*histogram.*'
        # Exclude debug metrics
      exclude:
        match_type: strict
        metric_names:
          - 'go_gc_duration_seconds'
          - 'go_goroutines'
          - 'process_virtual_memory_bytes'

  # Group by trace processor for performance
  groupbytrace:
    wait_duration: 10s
    num_traces: 10000

  # K8s attributes processor (if running in K8s)
  k8sattributes:
    auth_type: "serviceAccount"
    passthrough: false
    filter:
      node_from_env_var: KUBE_NODE_NAME
    extract:
      metadata:
        - k8s.pod.name
        - k8s.pod.uid
        - k8s.deployment.name
        - k8s.namespace.name
        - k8s.node.name
        - k8s.pod.start_time
      labels:
        - tag_name: app.label.component
          key: app.kubernetes.io/component
          from: pod
      annotations:
        - tag_name: app.annotation.version
          key: app.version
          from: pod

exporters:
  # Tempo exporter for traces
  otlp/tempo:
    endpoint: http://tempo:4317
    tls:
      insecure: true
    sending_queue:
      enabled: true
      num_consumers: 10
      queue_size: 5000
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 300s

  # Mimir exporter for metrics
  prometheusremotewrite/mimir:
    endpoint: http://mimir:8080/api/v1/push
    headers:
      X-Scope-OrgID: pyairtable
    tls:
      insecure: true
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 300s
    sending_queue:
      enabled: true
      num_consumers: 10
      queue_size: 5000

  # Loki exporter for logs
  loki:
    endpoint: http://loki:3100/loki/api/v1/push
    tenant_id: pyairtable
    tls:
      insecure: true
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 300s

  # Debug exporter for troubleshooting
  logging:
    loglevel: warn
    sampling_initial: 5
    sampling_thereafter: 200

  # Prometheus exporter for local metrics
  prometheus:
    endpoint: "0.0.0.0:8889"
    const_labels:
      environment: production
      cluster: pyairtable
    send_timestamps: true
    metric_expiration: 180s
    enable_open_metrics: true

service:
  pipelines:
    # Traces pipeline
    traces:
      receivers: [otlp, jaeger, zipkin]
      processors: 
        - memory_limiter
        - resource
        - attributes
        - span
        - tail_sampling
        - groupbytrace
        - batch
      exporters: [otlp/tempo, logging]

    # Metrics pipeline
    metrics:
      receivers: [otlp, prometheus, hostmetrics, docker_stats]
      processors:
        - memory_limiter
        - resource
        - transform
        - filter
        - batch
      exporters: [prometheusremotewrite/mimir, prometheus]

    # Logs pipeline
    logs:
      receivers: [otlp]
      processors:
        - memory_limiter
        - resource
        - batch
      exporters: [loki]

  # Extensions
  extensions: [health_check, pprof, zpages]

  # Telemetry
  telemetry:
    logs:
      level: warn
      development: false
    metrics:
      level: basic
      address: 0.0.0.0:8888

extensions:
  # Health check
  health_check:
    endpoint: 0.0.0.0:13133
    path: /

  # pprof for performance profiling
  pprof:
    endpoint: 0.0.0.0:1777

  # zpages for debugging
  zpages:
    endpoint: 0.0.0.0:55679
# Promtail Production Configuration
# Enhanced log collection with filtering, parsing, and multi-format support

server:
  http_listen_port: 9080
  grpc_listen_port: 0
  log_level: warn
  log_format: json

positions:
  filename: /tmp/positions/positions.yaml
  sync_period: 10s
  ignore_invalid_yaml: false

clients:
  - url: http://loki:3100/loki/api/v1/push
    batchwait: 1s
    batchsize: 1048576
    follow_redirects: false
    backoff_config:
      min_period: 500ms
      max_period: 5m
      max_retries: 10
    timeout: 10s

scrape_configs:
  # PyAirtable application logs
  - job_name: pyairtable-apps
    static_configs:
      - targets:
          - localhost
        labels:
          job: pyairtable-apps
          environment: production
          __path__: /var/lib/docker/containers/*/*-json.log

    pipeline_stages:
      # Parse Docker JSON logs
      - json:
          expressions:
            output: log
            stream: stream
            timestamp: time
            container_name: attrs.name

      # Extract container name
      - regex:
          expression: '^/(?P<container_name>[^/]+)$'
          source: container_name
          
      # Parse application logs based on service
      - match:
          selector: '{container_name=~".*pyairtable.*"}'
          stages:
            # Parse structured JSON logs
            - json:
                expressions:
                  level: level
                  message: message
                  service: service
                  trace_id: trace_id
                  span_id: span_id
                  user_id: user_id
                  request_id: request_id
                  duration: duration
                  status_code: status_code
                  method: method
                  path: path
                  error: error

            # Normalize log levels
            - template:
                source: level
                template: '{{ ToLower .Value }}'
                
            # Add service-specific labels
            - labels:
                level:
                service:
                container_name:
                
            # Drop noisy logs
            - drop:
                expression: '.*health.*check.*'
                older_than: 24h

  # System logs
  - job_name: system-logs
    static_configs:
      - targets:
          - localhost
        labels:
          job: system-logs
          environment: production
          __path__: /var/log/syslog

    pipeline_stages:
      # Parse syslog format
      - regex:
          expression: '^(?P<timestamp>\w+\s+\d+\s+\d+:\d+:\d+)\s+(?P<hostname>\S+)\s+(?P<service>\S+?)(\[(?P<pid>\d+)\])?:\s+(?P<message>.*)$'
          
      # Parse timestamp
      - timestamp:
          source: timestamp
          format: Jan 2 15:04:05
          location: UTC
          
      # Add labels
      - labels:
          hostname:
          service:
          
      # Filter out noisy messages
      - drop:
          expression: '.*(CRON|systemd).*'

  # Docker daemon logs
  - job_name: docker-logs
    static_configs:
      - targets:
          - localhost
        labels:
          job: docker-logs
          environment: production
          __path__: /var/log/docker.log

    pipeline_stages:
      # Parse Docker daemon logs
      - regex:
          expression: '^time="(?P<timestamp>[^"]+)"\s+level=(?P<level>\w+)\s+(?P<message>.*)$'
          
      # Parse timestamp
      - timestamp:
          source: timestamp
          format: RFC3339Nano
          
      # Add labels
      - labels:
          level:
          
      # Drop debug messages
      - drop:
          expression: '.*level=debug.*'

  # Nginx access logs (if applicable)
  - job_name: nginx-access
    static_configs:
      - targets:
          - localhost
        labels:
          job: nginx-access
          environment: production
          log_type: access
          __path__: /var/log/nginx/access.log

    pipeline_stages:
      # Parse nginx access logs
      - regex:
          expression: '^(?P<remote_addr>\S+)\s+-\s+(?P<remote_user>\S+)\s+\[(?P<time_local>[^\]]+)\]\s+"(?P<method>\S+)\s+(?P<path>\S+)\s+(?P<protocol>[^"]+)"\s+(?P<status>\d+)\s+(?P<body_bytes_sent>\d+)\s+"(?P<http_referer>[^"]*)"\s+"(?P<http_user_agent>[^"]*)"(?:\s+"(?P<http_x_forwarded_for>[^"]*)")?'
          
      # Parse timestamp
      - timestamp:
          source: time_local
          format: 02/Jan/2006:15:04:05 -0700
          
      # Add labels for filtering
      - labels:
          method:
          status:
          
      # Extract metrics
      - metrics:
          http_requests_total:
            type: Counter
            description: "Total HTTP requests"
            source: status
            config:
              action: inc
          http_request_duration_seconds:
            type: Histogram
            description: "HTTP request duration"
            source: body_bytes_sent
            config:
              buckets: [0.1, 0.5, 1.0, 5.0, 10.0]

  # Nginx error logs
  - job_name: nginx-error
    static_configs:
      - targets:
          - localhost
        labels:
          job: nginx-error
          environment: production
          log_type: error
          __path__: /var/log/nginx/error.log

    pipeline_stages:
      # Parse nginx error logs
      - regex:
          expression: '^(?P<timestamp>\d{4}/\d{2}/\d{2}\s+\d{2}:\d{2}:\d{2})\s+\[(?P<level>\w+)\]\s+(?P<message>.*)$'
          
      # Parse timestamp
      - timestamp:
          source: timestamp
          format: 2006/01/02 15:04:05
          
      # Add labels
      - labels:
          level:

  # PostgreSQL logs (if available)
  - job_name: postgresql-logs
    static_configs:
      - targets:
          - localhost
        labels:
          job: postgresql-logs
          environment: production
          __path__: /var/log/postgresql/*.log

    pipeline_stages:
      # Parse PostgreSQL logs
      - regex:
          expression: '^(?P<timestamp>\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}\.\d{3}\s+\w+)\s+\[(?P<pid>\d+)\]\s+(?P<level>\w+):\s+(?P<message>.*)$'
          
      # Parse timestamp
      - timestamp:
          source: timestamp
          format: 2006-01-02 15:04:05.000 MST
          
      # Add labels
      - labels:
          level:
          pid:
          
      # Filter slow queries for monitoring
      - match:
          selector: '{job="postgresql-logs"}'
          stages:
            - regex:
                expression: '.*duration:\s+(?P<duration>[\d.]+)\s+ms.*'
            - template:
                source: duration
                template: '{{ .Value }}'
            - labels:
                duration:

  # Application-specific log parsing for PyAirtable services
  - job_name: pyairtable-structured
    docker_sd_configs:
      - host: unix:///var/run/docker.sock
        refresh_interval: 5s
        filters:
          - name: name
            values: ["pyairtable-*"]

    relabel_configs:
      # Set service name from container name
      - source_labels: [__meta_docker_container_name]
        regex: '/pyairtable-(.+)'
        target_label: service
        replacement: '${1}'
        
      # Set environment
      - target_label: environment
        replacement: production
        
      # Set log path
      - source_labels: [__meta_docker_container_id]
        target_label: __path__
        replacement: '/var/lib/docker/containers/${1}/${1}-json.log'

    pipeline_stages:
      # Parse Docker JSON
      - json:
          expressions:
            output: log
            stream: stream
            timestamp: time

      # Parse structured application logs
      - json:
          expressions:
            level: level
            message: message
            service: service
            trace_id: trace_id
            span_id: span_id
            user_id: user_id
            request_id: request_id
            duration_ms: duration_ms
            status_code: status_code
            method: method
            path: path
            error: error
            cost_cents: cost_cents

      # Add trace correlation labels
      - labels:
          trace_id:
          span_id:
          
      # Extract business metrics
      - metrics:
          api_requests_total:
            type: Counter
            description: "Total API requests"
            source: status_code
            config:
              action: inc
          api_request_duration_seconds:
            type: Histogram
            description: "API request duration"
            source: duration_ms
            config:
              buckets: [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
          llm_cost_total:
            type: Counter
            description: "Total LLM cost in cents"
            source: cost_cents
            config:
              action: add

# Limits
limits_config:
  readline_rate: 10000
  readline_burst: 20000
  max_streams: 10000

# Target configuration
target_config:
  sync_period: 10s
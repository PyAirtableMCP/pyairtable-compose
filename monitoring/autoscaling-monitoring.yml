# Comprehensive Autoscaling Monitoring and Alerting for PyAirtable
# Advanced metrics, dashboards, and alert rules for all scaling components

# Prometheus configuration for autoscaling metrics
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'pyairtable'
    environment: 'production'

# Rule files for autoscaling alerts
rule_files:
  - "autoscaling_alert_rules.yml"
  - "cost_optimization_rules.yml"
  - "predictive_scaling_rules.yml"

# Scrape configurations for autoscaling components
scrape_configs:
  # Kubernetes API Server metrics
  - job_name: 'kubernetes-apiservers'
    kubernetes_sd_configs:
    - role: endpoints
    scheme: https
    tls_config:
      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    relabel_configs:
    - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
      action: keep
      regex: default;kubernetes;https

  # Kubernetes kubelet metrics
  - job_name: 'kubernetes-kubelet'
    kubernetes_sd_configs:
    - role: node
    scheme: https
    tls_config:
      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    relabel_configs:
    - action: labelmap
      regex: __meta_kubernetes_node_label_(.+)

  # Kubernetes cAdvisor metrics
  - job_name: 'kubernetes-cadvisor'
    kubernetes_sd_configs:
    - role: node
    scheme: https
    metrics_path: /metrics/cadvisor
    tls_config:
      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    relabel_configs:
    - action: labelmap
      regex: __meta_kubernetes_node_label_(.+)

  # KEDA metrics
  - job_name: 'keda-metrics'
    kubernetes_sd_configs:
    - role: endpoints
      namespaces:
        names:
        - keda
    relabel_configs:
    - source_labels: [__meta_kubernetes_service_name]
      action: keep
      regex: keda-operator-metrics-service
    - source_labels: [__meta_kubernetes_endpoint_port_name]
      action: keep
      regex: metrics

  # HPA metrics
  - job_name: 'hpa-metrics'
    kubernetes_sd_configs:
    - role: endpoints
    relabel_configs:
    - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
      action: keep
      regex: true
    - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
      action: replace
      target_label: __metrics_path__
      regex: (.+)

  # Custom metrics API
  - job_name: 'custom-metrics-api'
    kubernetes_sd_configs:
    - role: endpoints
      namespaces:
        names:
        - pyairtable-monitoring
    relabel_configs:
    - source_labels: [__meta_kubernetes_service_name]
      action: keep
      regex: custom-metrics-apiserver

  # Predictive scaling engine
  - job_name: 'predictive-scaling'
    kubernetes_sd_configs:
    - role: endpoints
      namespaces:
        names:
        - predictive-scaling
    relabel_configs:
    - source_labels: [__meta_kubernetes_service_name]
      action: keep
      regex: predictive-scaling-engine

  # Cost optimization controller
  - job_name: 'cost-optimization'
    kubernetes_sd_configs:
    - role: endpoints
      namespaces:
        names:
        - cost-optimization
    relabel_configs:
    - source_labels: [__meta_kubernetes_service_name]
      action: keep
      regex: cost-optimization-controller

  # Application services with autoscaling
  - job_name: 'pyairtable-services'
    kubernetes_sd_configs:
    - role: endpoints
      namespaces:
        names:
        - pyairtable
    relabel_configs:
    - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
      action: keep
      regex: true
    - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
      action: replace
      target_label: __metrics_path__
      regex: (.+)
    - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
      action: replace
      regex: ([^:]+)(?::\d+)?;(\d+)
      replacement: $1:$2
      target_label: __address__
    - action: labelmap
      regex: __meta_kubernetes_service_label_(.+)
    - source_labels: [__meta_kubernetes_namespace]
      action: replace
      target_label: kubernetes_namespace
    - source_labels: [__meta_kubernetes_service_name]
      action: replace
      target_label: kubernetes_name

  # Node exporter for infrastructure metrics
  - job_name: 'node-exporter'
    kubernetes_sd_configs:
    - role: endpoints
    relabel_configs:
    - source_labels: [__meta_kubernetes_endpoints_name]
      regex: 'node-exporter'
      action: keep

  # AWS CloudWatch metrics via CloudWatch Exporter
  - job_name: 'cloudwatch-exporter'
    static_configs:
    - targets: ['cloudwatch-exporter:9106']
    scrape_interval: 60s
    metrics_path: /metrics

---
# Autoscaling Alert Rules
apiVersion: v1
kind: ConfigMap
metadata:
  name: autoscaling-alert-rules
  namespace: pyairtable-monitoring
data:
  autoscaling_alert_rules.yml: |
    groups:
    - name: autoscaling.rules
      rules:
      
      # HPA Scaling Alerts
      - alert: HPAMaxReplicasReached
        expr: kube_horizontalpodautoscaler_status_current_replicas == kube_horizontalpodautoscaler_spec_max_replicas
        for: 5m
        labels:
          severity: warning
          component: autoscaling
        annotations:
          summary: "HPA {{ $labels.horizontalpodautoscaler }} has reached maximum replicas"
          description: "HPA {{ $labels.horizontalpodautoscaler }} in namespace {{ $labels.namespace }} has reached its maximum replica count of {{ $value }}. Consider increasing the max replicas or investigating high load."
      
      - alert: HPATargetMetricUnavailable
        expr: kube_horizontalpodautoscaler_status_condition{condition="ScalingActive", status="false"} == 1
        for: 2m
        labels:
          severity: critical
          component: autoscaling
        annotations:
          summary: "HPA {{ $labels.horizontalpodautoscaler }} cannot access target metrics"
          description: "HPA {{ $labels.horizontalpodautoscaler }} in namespace {{ $labels.namespace }} cannot access its target metrics, scaling is disabled."
      
      - alert: HPAScalingDisabled
        expr: kube_horizontalpodautoscaler_status_condition{condition="AbleToScale", status="false"} == 1
        for: 5m
        labels:
          severity: warning
          component: autoscaling
        annotations:
          summary: "HPA {{ $labels.horizontalpodautoscaler }} scaling is disabled"
          description: "HPA {{ $labels.horizontalpodautoscaler }} in namespace {{ $labels.namespace }} is unable to scale."
      
      # KEDA Scaling Alerts
      - alert: KEDAScalerError
        expr: keda_scaler_errors_total > 0
        for: 1m
        labels:
          severity: critical
          component: autoscaling
        annotations:
          summary: "KEDA scaler {{ $labels.scaler }} has errors"
          description: "KEDA scaler {{ $labels.scaler }} for ScaledObject {{ $labels.scaledObject }} in namespace {{ $labels.namespace }} is experiencing errors."
      
      - alert: KEDAMaxReplicasReached
        expr: keda_scaled_object_replicas == keda_scaled_object_max_replicas
        for: 5m
        labels:
          severity: warning
          component: autoscaling
        annotations:
          summary: "KEDA ScaledObject {{ $labels.scaledObject }} has reached maximum replicas"
          description: "KEDA ScaledObject {{ $labels.scaledObject }} in namespace {{ $labels.namespace }} has reached its maximum replica count."
      
      - alert: KEDAScaleToZeroStuck
        expr: keda_scaled_object_replicas > 0 and keda_scaler_metric_value == 0
        for: 30m
        labels:
          severity: warning
          component: autoscaling
        annotations:
          summary: "KEDA ScaledObject {{ $labels.scaledObject }} stuck with replicas despite zero metrics"
          description: "KEDA ScaledObject {{ $labels.scaledObject }} has {{ $value }} replicas but metrics show zero value for 30 minutes."
      
      # VPA Alerts
      - alert: VPARecommendationNotApplied
        expr: |
          (
            kube_verticalpodautoscaler_status_recommendation_containerrecommendations_target{resource="cpu"} -
            kube_pod_container_resource_requests{resource="cpu"}
          ) / kube_pod_container_resource_requests{resource="cpu"} > 0.5
        for: 24h
        labels:
          severity: info
          component: autoscaling
        annotations:
          summary: "VPA recommendation significantly different from current requests"
          description: "VPA {{ $labels.verticalpodautoscaler }} recommends {{ $value | humanizePercentage }} more CPU than currently requested for container {{ $labels.container }}."
      
      # Cluster Autoscaler Alerts
      - alert: ClusterAutoscalerUnschedulablePods
        expr: cluster_autoscaler_unschedulable_pods_count > 0
        for: 10m
        labels:
          severity: warning
          component: autoscaling
        annotations:
          summary: "Cluster Autoscaler has unschedulable pods"
          description: "There are {{ $value }} unschedulable pods. The cluster autoscaler may need attention."
      
      - alert: ClusterAutoscalerScaleUpFailed
        expr: increase(cluster_autoscaler_failed_scale_ups_total[10m]) > 0
        labels:
          severity: critical
          component: autoscaling
        annotations:
          summary: "Cluster Autoscaler failed to scale up"
          description: "Cluster Autoscaler failed to scale up {{ $value }} times in the last 10 minutes."
      
      # Resource Saturation Alerts
      - alert: HighCPUUtilizationAcrossCluster
        expr: avg(100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)) > 80
        for: 10m
        labels:
          severity: warning
          component: autoscaling
        annotations:
          summary: "High CPU utilization across cluster"
          description: "Average CPU utilization across the cluster is {{ $value | printf "%.2f" }}% for 10 minutes."
      
      - alert: HighMemoryUtilizationAcrossCluster
        expr: avg((1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100) > 85
        for: 10m
        labels:
          severity: warning
          component: autoscaling
        annotations:
          summary: "High memory utilization across cluster"
          description: "Average memory utilization across the cluster is {{ $value | printf "%.2f" }}% for 10 minutes."
      
      # Scaling Velocity Alerts
      - alert: RapidScalingUp
        expr: increase(kube_deployment_status_replicas[5m]) > 10
        labels:
          severity: info
          component: autoscaling
        annotations:
          summary: "Rapid scaling up detected for {{ $labels.deployment }}"
          description: "Deployment {{ $labels.deployment }} in namespace {{ $labels.namespace }} scaled up by {{ $value }} replicas in 5 minutes."
      
      - alert: RapidScalingDown
        expr: decrease(kube_deployment_status_replicas[5m]) > 10
        labels:
          severity: info
          component: autoscaling
        annotations:
          summary: "Rapid scaling down detected for {{ $labels.deployment }}"
          description: "Deployment {{ $labels.deployment }} in namespace {{ $labels.namespace }} scaled down by {{ $value }} replicas in 5 minutes."
      
      # Cost Optimization Alerts
      - alert: HighInfrastructureCost
        expr: pyairtable_estimated_hourly_cost > 25
        for: 1h
        labels:
          severity: warning
          component: cost-optimization
        annotations:
          summary: "Infrastructure cost is above normal"
          description: "Current estimated hourly cost is ${{ $value | printf "%.2f" }}, which is above the normal threshold."
      
      - alert: SpotInstanceTerminationHigh
        expr: increase(aws_ec2_spot_instance_terminations_total[1h]) > 5
        labels:
          severity: warning
          component: cost-optimization
        annotations:
          summary: "High rate of spot instance terminations"
          description: "{{ $value }} spot instances were terminated in the last hour."
      
      # Predictive Scaling Alerts
      - alert: PredictiveScalingModelAccuracyLow
        expr: predictive_scaling_model_accuracy < 0.7
        for: 30m
        labels:
          severity: warning
          component: predictive-scaling
        annotations:
          summary: "Predictive scaling model accuracy is low"
          description: "Model accuracy for service {{ $labels.service }} is {{ $value | printf "%.2f" }}, which is below the acceptable threshold."
      
      - alert: PredictiveScalingPredictionStale
        expr: time() - predictive_scaling_last_prediction_timestamp > 1800
        labels:
          severity: critical
          component: predictive-scaling
        annotations:
          summary: "Predictive scaling predictions are stale"
          description: "No new predictions have been generated for {{ $labels.service }} in the last 30 minutes."

  cost_optimization_rules.yml: |
    groups:
    - name: cost.optimization.rules
      rules:
      
      # Cost Tracking Rules
      - record: pyairtable:cost_per_replica_hourly
        expr: |
          (
            kube_pod_container_resource_requests{resource="cpu"} * 0.048 +  # $0.048 per vCPU hour
            kube_pod_container_resource_requests{resource="memory"} / (1024*1024*1024) * 0.006  # $0.006 per GB hour
          )
      
      - record: pyairtable:total_hourly_cost
        expr: sum(pyairtable:cost_per_replica_hourly)
      
      - record: pyairtable:cost_by_service
        expr: sum(pyairtable:cost_per_replica_hourly) by (app, namespace)
      
      # Efficiency Metrics
      - record: pyairtable:resource_efficiency_cpu
        expr: |
          sum(rate(container_cpu_usage_seconds_total[5m])) by (pod) /
          sum(kube_pod_container_resource_requests{resource="cpu"}) by (pod)
      
      - record: pyairtable:resource_efficiency_memory
        expr: |
          sum(container_memory_usage_bytes) by (pod) /
          sum(kube_pod_container_resource_requests{resource="memory"}) by (pod)
      
      # Waste Detection
      - record: pyairtable:cpu_waste_percentage
        expr: |
          (1 - pyairtable:resource_efficiency_cpu) * 100
      
      - record: pyairtable:memory_waste_percentage
        expr: |
          (1 - pyairtable:resource_efficiency_memory) * 100
      
      # Spot Instance Savings
      - record: pyairtable:spot_instance_savings_hourly
        expr: |
          (
            count(kube_node_info{node_type="on-demand"}) * 0.096 -  # On-demand cost
            count(kube_node_info{node_type="spot"}) * 0.029        # Spot cost (70% savings)
          )

  predictive_scaling_rules.yml: |
    groups:
    - name: predictive.scaling.rules
      rules:
      
      # Model Performance Metrics
      - record: predictive_scaling:model_accuracy_avg
        expr: avg(predictive_scaling_model_accuracy) by (service, model_type)
      
      - record: predictive_scaling:prediction_error_rate
        expr: |
          abs(predictive_scaling_predicted_value - predictive_scaling_actual_value) /
          predictive_scaling_actual_value
      
      # Scaling Decision Tracking
      - record: predictive_scaling:scaling_decisions_per_hour
        expr: sum(increase(predictive_scaling_decisions_total[1h])) by (service, action)
      
      - record: predictive_scaling:scaling_accuracy
        expr: |
          sum(predictive_scaling_correct_decisions_total) /
          sum(predictive_scaling_total_decisions)
      
      # Resource Prediction Accuracy
      - record: predictive_scaling:cpu_prediction_accuracy
        expr: |
          1 - abs(
            predictive_scaling_predicted_cpu_usage - 
            avg(rate(container_cpu_usage_seconds_total[5m]))
          ) / avg(rate(container_cpu_usage_seconds_total[5m]))
      
      - record: predictive_scaling:memory_prediction_accuracy
        expr: |
          1 - abs(
            predictive_scaling_predicted_memory_usage - 
            avg(container_memory_usage_bytes)
          ) / avg(container_memory_usage_bytes)

---
# Grafana Dashboard for Autoscaling Monitoring
apiVersion: v1
kind: ConfigMap
metadata:
  name: autoscaling-dashboard
  namespace: pyairtable-monitoring
data:
  autoscaling-dashboard.json: |
    {
      "dashboard": {
        "id": null,
        "title": "PyAirtable Autoscaling Monitoring",
        "tags": ["autoscaling", "kubernetes", "pyairtable"],
        "timezone": "browser",
        "panels": [
          {
            "id": 1,
            "title": "HPA Status Overview",
            "type": "stat",
            "targets": [
              {
                "expr": "count(kube_horizontalpodautoscaler_info)",
                "legendFormat": "Total HPAs"
              },
              {
                "expr": "count(kube_horizontalpodautoscaler_status_current_replicas == kube_horizontalpodautoscaler_spec_max_replicas)",
                "legendFormat": "At Max Replicas"
              }
            ],
            "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0}
          },
          {
            "id": 2,
            "title": "KEDA ScaledObjects Status",
            "type": "stat",
            "targets": [
              {
                "expr": "count(keda_scaled_object_info)",
                "legendFormat": "Total ScaledObjects"
              },
              {
                "expr": "count(keda_scaled_object_replicas == keda_scaled_object_max_replicas)",
                "legendFormat": "At Max Replicas"
              }
            ],
            "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0}
          },
          {
            "id": 3,
            "title": "Scaling Events Timeline",
            "type": "graph",
            "targets": [
              {
                "expr": "sum(increase(kube_deployment_status_replicas[5m])) by (deployment)",
                "legendFormat": "{{ deployment }} scale events"
              }
            ],
            "gridPos": {"h": 8, "w": 24, "x": 0, "y": 8}
          },
          {
            "id": 4,
            "title": "Resource Utilization vs Scaling Thresholds",
            "type": "graph",
            "targets": [
              {
                "expr": "avg(rate(container_cpu_usage_seconds_total[5m])) by (pod) * 100",
                "legendFormat": "CPU Usage %"
              },
              {
                "expr": "avg(container_memory_usage_bytes) by (pod) / avg(kube_pod_container_resource_limits{resource=\"memory\"}) by (pod) * 100",
                "legendFormat": "Memory Usage %"
              }
            ],
            "gridPos": {"h": 8, "w": 24, "x": 0, "y": 16}
          },
          {
            "id": 5,
            "title": "Cost Optimization Metrics",
            "type": "graph",
            "targets": [
              {
                "expr": "pyairtable:total_hourly_cost",
                "legendFormat": "Hourly Cost ($)"
              },
              {
                "expr": "pyairtable:spot_instance_savings_hourly",
                "legendFormat": "Spot Savings ($)"
              }
            ],
            "gridPos": {"h": 8, "w": 12, "x": 0, "y": 24}
          },
          {
            "id": 6,
            "title": "Predictive Scaling Accuracy",
            "type": "graph",
            "targets": [
              {
                "expr": "predictive_scaling:model_accuracy_avg",
                "legendFormat": "{{ service }} accuracy"
              }
            ],
            "gridPos": {"h": 8, "w": 12, "x": 12, "y": 24}
          }
        ],
        "time": {
          "from": "now-6h",
          "to": "now"
        },
        "refresh": "30s"
      }
    }

---
# AlertManager configuration for autoscaling alerts
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-autoscaling-config
  namespace: pyairtable-monitoring
data:
  alertmanager.yml: |
    global:
      slack_api_url: 'YOUR_SLACK_WEBHOOK_URL'
      
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'web.hook'
      routes:
      - match:
          component: autoscaling
        receiver: 'autoscaling-alerts'
      - match:
          component: cost-optimization
        receiver: 'cost-alerts'
      - match:
          component: predictive-scaling
        receiver: 'predictive-scaling-alerts'
    
    receivers:
    - name: 'web.hook'
      webhook_configs:
      - url: 'http://localhost:5001/'
    
    - name: 'autoscaling-alerts'
      slack_configs:
      - channel: '#infrastructure-alerts'
        title: 'PyAirtable Autoscaling Alert'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Severity:* {{ .Labels.severity }}
          {{ end }}
    
    - name: 'cost-alerts'
      slack_configs:
      - channel: '#cost-optimization'
        title: 'PyAirtable Cost Alert'
        text: |
          {{ range .Alerts }}
          *Cost Alert:* {{ .Annotations.summary }}
          *Details:* {{ .Annotations.description }}
          {{ end }}
      email_configs:
      - to: 'finance@company.com'
        subject: 'PyAirtable Cost Alert'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          {{ end }}
    
    - name: 'predictive-scaling-alerts'
      slack_configs:
      - channel: '#ml-ops'
        title: 'PyAirtable Predictive Scaling Alert'
        text: |
          {{ range .Alerts }}
          *ML Alert:* {{ .Annotations.summary }}
          *Details:* {{ .Annotations.description }}
          {{ end }}

---
# ServiceMonitor for comprehensive autoscaling metrics
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: autoscaling-comprehensive-metrics
  namespace: pyairtable-monitoring
  labels:
    app: pyairtable-autoscaling
spec:
  selector:
    matchLabels:
      app.kubernetes.io/component: autoscaling
  namespaceSelector:
    any: true
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
  - port: http
    interval: 30s
    path: /metrics
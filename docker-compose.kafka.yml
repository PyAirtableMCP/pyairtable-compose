version: '3.8'

# Apache Kafka Event Streaming Platform for PyAirtable
# Production-ready multi-broker setup with Schema Registry, Kafka Connect, and KSQL

services:
  # =============================================================================
  # ZOOKEEPER ENSEMBLE
  # =============================================================================
  
  zookeeper-1:
    image: confluentinc/cp-zookeeper:7.5.0
    hostname: zookeeper-1
    container_name: zookeeper-1
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_SERVERS: zookeeper-1:2888:3888;zookeeper-2:2888:3888;zookeeper-3:2888:3888
      ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT: 3
      ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL: 24
      ZOOKEEPER_4LW_COMMANDS_WHITELIST: mntr,conf,ruok
      ZOOKEEPER_JMX_HOSTNAME: zookeeper-1
      ZOOKEEPER_JMX_PORT: 9999
      ZOOKEEPER_LOG4J_ROOT_LOGLEVEL: INFO
    volumes:
      - zookeeper-1-data:/var/lib/zookeeper/data
      - zookeeper-1-logs:/var/lib/zookeeper/log
    networks:
      - kafka-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "bash", "-c", "echo 'ruok' | nc localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M

  zookeeper-2:
    image: confluentinc/cp-zookeeper:7.5.0
    hostname: zookeeper-2
    container_name: zookeeper-2
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_SERVER_ID: 2
      ZOOKEEPER_SERVERS: zookeeper-1:2888:3888;zookeeper-2:2888:3888;zookeeper-3:2888:3888
      ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT: 3
      ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL: 24
      ZOOKEEPER_4LW_COMMANDS_WHITELIST: mntr,conf,ruok
      ZOOKEEPER_JMX_HOSTNAME: zookeeper-2
      ZOOKEEPER_JMX_PORT: 9999
      ZOOKEEPER_LOG4J_ROOT_LOGLEVEL: INFO
    volumes:
      - zookeeper-2-data:/var/lib/zookeeper/data
      - zookeeper-2-logs:/var/lib/zookeeper/log
    networks:
      - kafka-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "bash", "-c", "echo 'ruok' | nc localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M

  zookeeper-3:
    image: confluentinc/cp-zookeeper:7.5.0
    hostname: zookeeper-3
    container_name: zookeeper-3
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_SERVER_ID: 3
      ZOOKEEPER_SERVERS: zookeeper-1:2888:3888;zookeeper-2:2888:3888;zookeeper-3:2888:3888
      ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT: 3
      ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL: 24
      ZOOKEEPER_4LW_COMMANDS_WHITELIST: mntr,conf,ruok
      ZOOKEEPER_JMX_HOSTNAME: zookeeper-3
      ZOOKEEPER_JMX_PORT: 9999
      ZOOKEEPER_LOG4J_ROOT_LOGLEVEL: INFO
    volumes:
      - zookeeper-3-data:/var/lib/zookeeper/data
      - zookeeper-3-logs:/var/lib/zookeeper/log
    networks:
      - kafka-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "bash", "-c", "echo 'ruok' | nc localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M

  # =============================================================================
  # KAFKA BROKER CLUSTER WITH SECURITY
  # =============================================================================

  kafka-1:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka-1
    container_name: kafka-1
    depends_on:
      zookeeper-1:
        condition: service_healthy
      zookeeper-2:
        condition: service_healthy
      zookeeper-3:
        condition: service_healthy
    ports:
      - "9092:9092"
      - "9093:9093"  # SSL port
      - "9094:9094"  # SASL_SSL port
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      
      # Multiple listener configurations for development flexibility
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_SSL:SASL_SSL,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-1:29092,SSL://kafka-1:29093,SASL_SSL://kafka-1:29094,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,SSL://0.0.0.0:29093,SASL_SSL://0.0.0.0:29094,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: SSL
      
      # SSL Configuration
      KAFKA_SSL_KEYSTORE_FILENAME: kafka.server.keystore.jks
      KAFKA_SSL_KEYSTORE_CREDENTIALS: kafka_keystore_creds
      KAFKA_SSL_KEY_CREDENTIALS: kafka_ssl_key_creds
      KAFKA_SSL_TRUSTSTORE_FILENAME: kafka.server.truststore.jks
      KAFKA_SSL_TRUSTSTORE_CREDENTIALS: kafka_truststore_creds
      KAFKA_SSL_CLIENT_AUTH: none
      KAFKA_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: ""
      
      # SASL Configuration
      KAFKA_SASL_ENABLED_MECHANISMS: SCRAM-SHA-256,SCRAM-SHA-512
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: SCRAM-SHA-256
      
      # SASL JAAS Configuration
      KAFKA_OPTS: "-Djava.security.auth.login.config=/etc/kafka/secrets/kafka_server_jaas.conf"
      
      # Cluster Configuration
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_NUM_PARTITIONS: 12
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      
      # Performance Optimizations
      KAFKA_NUM_NETWORK_THREADS: 8
      KAFKA_NUM_IO_THREADS: 8
      KAFKA_SOCKET_SEND_BUFFER_BYTES: 102400
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 102400
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 104857600
      KAFKA_MESSAGE_MAX_BYTES: 10485760
      KAFKA_REPLICA_FETCH_MAX_BYTES: 10485760
      
      # Log Configuration
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_RETENTION_BYTES: 1073741824
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_CLEANUP_POLICY: delete
      KAFKA_LOG_FLUSH_INTERVAL_MS: 1000
      KAFKA_LOG_FLUSH_INTERVAL_MESSAGES: 10000
      
      # Compression
      KAFKA_COMPRESSION_TYPE: snappy
      
      # Auto Topic Creation (disabled for production control)
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
      KAFKA_DELETE_TOPIC_ENABLE: 'true'
      
      # Producer/Consumer Optimizations
      KAFKA_BATCH_SIZE: 16384
      KAFKA_LINGER_MS: 5
      KAFKA_BUFFER_MEMORY: 33554432
      KAFKA_MAX_REQUEST_SIZE: 1048576
      
      # JMX Configuration
      KAFKA_JMX_HOSTNAME: kafka-1
      KAFKA_JMX_PORT: 9101
      JMX_PORT: 9101
      
      # Heap Configuration
      KAFKA_HEAP_OPTS: "-Xmx1G -Xms1G"
      
      # Log Level
      KAFKA_LOG4J_ROOT_LOGLEVEL: INFO
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      
    volumes:
      - kafka-1-data:/var/lib/kafka/data
      - ./kafka-security:/etc/kafka/secrets
    networks:
      - kafka-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "kafka-1:29092"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1.5G
        reservations:
          cpus: '0.5'
          memory: 1G

  kafka-2:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka-2
    container_name: kafka-2
    depends_on:
      zookeeper-1:
        condition: service_healthy
      zookeeper-2:
        condition: service_healthy
      zookeeper-3:
        condition: service_healthy
    ports:
      - "9095:9095"
      - "9096:9096"  # SSL port
      - "9097:9097"  # SASL_SSL port
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      
      # Multiple listener configurations for development flexibility
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_SSL:SASL_SSL,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-2:29095,SSL://kafka-2:29096,SASL_SSL://kafka-2:29097,PLAINTEXT_HOST://localhost:9095
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29095,SSL://0.0.0.0:29096,SASL_SSL://0.0.0.0:29097,PLAINTEXT_HOST://0.0.0.0:9095
      KAFKA_INTER_BROKER_LISTENER_NAME: SSL
      
      # SSL Configuration
      KAFKA_SSL_KEYSTORE_FILENAME: kafka.server.keystore.jks
      KAFKA_SSL_KEYSTORE_CREDENTIALS: kafka_keystore_creds
      KAFKA_SSL_KEY_CREDENTIALS: kafka_ssl_key_creds
      KAFKA_SSL_TRUSTSTORE_FILENAME: kafka.server.truststore.jks
      KAFKA_SSL_TRUSTSTORE_CREDENTIALS: kafka_truststore_creds
      KAFKA_SSL_CLIENT_AUTH: none
      KAFKA_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: ""
      
      # SASL Configuration
      KAFKA_SASL_ENABLED_MECHANISMS: SCRAM-SHA-256,SCRAM-SHA-512
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: SCRAM-SHA-256
      
      # SASL JAAS Configuration
      KAFKA_OPTS: "-Djava.security.auth.login.config=/etc/kafka/secrets/kafka_server_jaas.conf"
      
      # Cluster Configuration
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_NUM_PARTITIONS: 12
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      
      # Performance Optimizations
      KAFKA_NUM_NETWORK_THREADS: 8
      KAFKA_NUM_IO_THREADS: 8
      KAFKA_SOCKET_SEND_BUFFER_BYTES: 102400
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 102400
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 104857600
      KAFKA_MESSAGE_MAX_BYTES: 10485760
      KAFKA_REPLICA_FETCH_MAX_BYTES: 10485760
      
      # Log Configuration
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_RETENTION_BYTES: 1073741824
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_CLEANUP_POLICY: delete
      KAFKA_LOG_FLUSH_INTERVAL_MS: 1000
      KAFKA_LOG_FLUSH_INTERVAL_MESSAGES: 10000
      
      # Compression
      KAFKA_COMPRESSION_TYPE: snappy
      
      # Auto Topic Creation (disabled for production control)
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
      KAFKA_DELETE_TOPIC_ENABLE: 'true'
      
      # Producer/Consumer Optimizations
      KAFKA_BATCH_SIZE: 16384
      KAFKA_LINGER_MS: 5
      KAFKA_BUFFER_MEMORY: 33554432
      KAFKA_MAX_REQUEST_SIZE: 1048576
      
      # JMX Configuration
      KAFKA_JMX_HOSTNAME: kafka-2
      KAFKA_JMX_PORT: 9102
      JMX_PORT: 9102
      
      # Heap Configuration
      KAFKA_HEAP_OPTS: "-Xmx1G -Xms1G"
      
      # Log Level
      KAFKA_LOG4J_ROOT_LOGLEVEL: INFO
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      
    volumes:
      - kafka-2-data:/var/lib/kafka/data
      - ./kafka-security:/etc/kafka/secrets
    networks:
      - kafka-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "kafka-2:29093"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1.5G
        reservations:
          cpus: '0.5'
          memory: 1G

  kafka-3:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka-3
    container_name: kafka-3
    depends_on:
      zookeeper-1:
        condition: service_healthy
      zookeeper-2:
        condition: service_healthy
      zookeeper-3:
        condition: service_healthy
    ports:
      - "9098:9098"
      - "9099:9099"  # SSL port
      - "9100:9100"  # SASL_SSL port
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      
      # Multiple listener configurations for development flexibility
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_SSL:SASL_SSL,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-3:29098,SSL://kafka-3:29099,SASL_SSL://kafka-3:29100,PLAINTEXT_HOST://localhost:9098
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29098,SSL://0.0.0.0:29099,SASL_SSL://0.0.0.0:29100,PLAINTEXT_HOST://0.0.0.0:9098
      KAFKA_INTER_BROKER_LISTENER_NAME: SSL
      
      # SSL Configuration
      KAFKA_SSL_KEYSTORE_FILENAME: kafka.server.keystore.jks
      KAFKA_SSL_KEYSTORE_CREDENTIALS: kafka_keystore_creds
      KAFKA_SSL_KEY_CREDENTIALS: kafka_ssl_key_creds
      KAFKA_SSL_TRUSTSTORE_FILENAME: kafka.server.truststore.jks
      KAFKA_SSL_TRUSTSTORE_CREDENTIALS: kafka_truststore_creds
      KAFKA_SSL_CLIENT_AUTH: none
      KAFKA_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: ""
      
      # SASL Configuration
      KAFKA_SASL_ENABLED_MECHANISMS: SCRAM-SHA-256,SCRAM-SHA-512
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: SCRAM-SHA-256
      
      # SASL JAAS Configuration
      KAFKA_OPTS: "-Djava.security.auth.login.config=/etc/kafka/secrets/kafka_server_jaas.conf"
      
      # Cluster Configuration
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_NUM_PARTITIONS: 12
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      
      # Performance Optimizations
      KAFKA_NUM_NETWORK_THREADS: 8
      KAFKA_NUM_IO_THREADS: 8
      KAFKA_SOCKET_SEND_BUFFER_BYTES: 102400
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 102400
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 104857600
      KAFKA_MESSAGE_MAX_BYTES: 10485760
      KAFKA_REPLICA_FETCH_MAX_BYTES: 10485760
      
      # Log Configuration
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_RETENTION_BYTES: 1073741824
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_CLEANUP_POLICY: delete
      KAFKA_LOG_FLUSH_INTERVAL_MS: 1000
      KAFKA_LOG_FLUSH_INTERVAL_MESSAGES: 10000
      
      # Compression
      KAFKA_COMPRESSION_TYPE: snappy
      
      # Auto Topic Creation (disabled for production control)
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
      KAFKA_DELETE_TOPIC_ENABLE: 'true'
      
      # Producer/Consumer Optimizations
      KAFKA_BATCH_SIZE: 16384
      KAFKA_LINGER_MS: 5
      KAFKA_BUFFER_MEMORY: 33554432
      KAFKA_MAX_REQUEST_SIZE: 1048576
      
      # JMX Configuration
      KAFKA_JMX_HOSTNAME: kafka-3
      KAFKA_JMX_PORT: 9103
      JMX_PORT: 9103
      
      # Heap Configuration
      KAFKA_HEAP_OPTS: "-Xmx1G -Xms1G"
      
      # Log Level
      KAFKA_LOG4J_ROOT_LOGLEVEL: INFO
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      
    volumes:
      - kafka-3-data:/var/lib/kafka/data
      - ./kafka-security:/etc/kafka/secrets
    networks:
      - kafka-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "kafka-3:29094"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1.5G
        reservations:
          cpus: '0.5'
          memory: 1G

  # =============================================================================
  # SCHEMA REGISTRY
  # =============================================================================

  schema-registry:
    image: confluentinc/cp-schema-registry:7.5.0
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      kafka-1:
        condition: service_healthy
      kafka-2:
        condition: service_healthy
      kafka-3:
        condition: service_healthy
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka-1:29092,kafka-2:29093,kafka-3:29094
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
      SCHEMA_REGISTRY_DEBUG: 'false'
      
      # Schema Registry Configuration
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC: _schemas
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC_REPLICATION_FACTOR: 3
      SCHEMA_REGISTRY_LEADER_ELIGIBILITY: 'true'
      SCHEMA_REGISTRY_MODE_MUTABILITY: 'true'
      SCHEMA_REGISTRY_COMPATIBILITY_LEVEL: BACKWARD
      
      # Security
      SCHEMA_REGISTRY_SCHEMA_REGISTRY_INTER_INSTANCE_PROTOCOL: http
      
      # Access Control
      SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_ORIGIN: '*'
      SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_METHODS: GET,POST,PUT,DELETE,OPTIONS
      
      # JMX
      SCHEMA_REGISTRY_JMX_HOSTNAME: schema-registry
      SCHEMA_REGISTRY_JMX_PORT: 9582
      
      # Heap
      SCHEMA_REGISTRY_HEAP_OPTS: "-Xmx512M -Xms512M"
      
      # Log Level
      SCHEMA_REGISTRY_LOG4J_ROOT_LOGLEVEL: INFO
      
    networks:
      - kafka-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://schema-registry:8081/subjects"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 768M
        reservations:
          cpus: '0.25'
          memory: 512M

  # =============================================================================
  # KAFKA CONNECT
  # =============================================================================

  kafka-connect:
    image: confluentinc/cp-kafka-connect:7.5.0
    hostname: kafka-connect
    container_name: kafka-connect
    depends_on:
      kafka-1:
        condition: service_healthy
      kafka-2:
        condition: service_healthy
      kafka-3:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka-1:29092,kafka-2:29093,kafka-3:29094
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: pyairtable-connect-cluster
      
      # Topic Configuration
      CONNECT_CONFIG_STORAGE_TOPIC: pyairtable.connect.configs
      CONNECT_OFFSET_STORAGE_TOPIC: pyairtable.connect.offsets
      CONNECT_STATUS_STORAGE_TOPIC: pyairtable.connect.status
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 3
      
      # Converters
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      
      # Internal Converters
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      
      # Plugin Configuration
      CONNECT_PLUGIN_PATH: /usr/share/java,/usr/share/confluent-hub-components
      
      # Worker Configuration
      CONNECT_TASK_SHUTDOWN_GRACEFUL_TIMEOUT_MS: 30000
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      
      # Producer Configuration
      CONNECT_PRODUCER_BOOTSTRAP_SERVERS: kafka-1:29092,kafka-2:29093,kafka-3:29094
      CONNECT_PRODUCER_COMPRESSION_TYPE: snappy
      CONNECT_PRODUCER_ACKS: all
      CONNECT_PRODUCER_RETRIES: 2147483647
      CONNECT_PRODUCER_MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION: 5
      CONNECT_PRODUCER_ENABLE_IDEMPOTENCE: 'true'
      
      # Consumer Configuration
      CONNECT_CONSUMER_BOOTSTRAP_SERVERS: kafka-1:29092,kafka-2:29093,kafka-3:29094
      CONNECT_CONSUMER_AUTO_OFFSET_RESET: earliest
      CONNECT_CONSUMER_ENABLE_AUTO_COMMIT: 'false'
      CONNECT_CONSUMER_ISOLATION_LEVEL: read_committed
      
      # JMX
      CONNECT_JMX_HOSTNAME: kafka-connect
      CONNECT_JMX_PORT: 9584
      
      # Heap
      KAFKA_HEAP_OPTS: "-Xmx1G -Xms1G"
      
      # Log Level
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
      CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR
      
    volumes:
      - kafka-connect-data:/tmp/connect-configs
    networks:
      - kafka-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://kafka-connect:8083/connectors"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 90s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1.5G
        reservations:
          cpus: '0.5'
          memory: 1G

  # =============================================================================
  # KSQLDB SERVER
  # =============================================================================

  ksqldb-server:
    image: confluentinc/cp-ksqldb-server:7.5.0
    hostname: ksqldb-server
    container_name: ksqldb-server
    depends_on:
      kafka-1:
        condition: service_healthy
      kafka-2:
        condition: service_healthy
      kafka-3:
        condition: service_healthy
      kafka-connect:
        condition: service_healthy
    ports:
      - "8088:8088"
    environment:
      KSQL_CONFIG_DIR: "/etc/ksql"
      KSQL_BOOTSTRAP_SERVERS: kafka-1:29092,kafka-2:29093,kafka-3:29094
      KSQL_HOST_NAME: ksqldb-server
      KSQL_LISTENERS: "http://0.0.0.0:8088"
      KSQL_CACHE_MAX_BYTES_BUFFERING: 0
      KSQL_KSQL_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
      KSQL_PRODUCER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor"
      KSQL_CONSUMER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor"
      KSQL_KSQL_CONNECT_URL: "http://kafka-connect:8083"
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_REPLICATION_FACTOR: 3
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: 'true'
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: 'true'
      
      # Stream Processing Configuration
      KSQL_KSQL_STREAMS_REPLICATION_FACTOR: 3
      KSQL_KSQL_STREAMS_NUM_STREAM_THREADS: 4
      KSQL_KSQL_STREAMS_COMMIT_INTERVAL_MS: 2000
      KSQL_KSQL_STREAMS_CACHE_MAX_BYTES_BUFFERING: 10000000
      KSQL_KSQL_STREAMS_AUTO_OFFSET_RESET: earliest
      
      # Security
      KSQL_KSQL_SERVICE_ID: pyairtable-ksql-cluster
      
      # JMX
      KSQL_JMX_HOSTNAME: ksqldb-server
      KSQL_JMX_PORT: 9585
      
      # Heap
      KSQL_HEAP_OPTS: "-Xmx1G -Xms1G"
      
      # Log Level
      KSQL_LOG4J_ROOT_LOGLEVEL: INFO
      
    volumes:
      - ksqldb-data:/opt/ksqldb-udfs
    networks:
      - kafka-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://ksqldb-server:8088/info"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 90s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1.5G
        reservations:
          cpus: '0.5'
          memory: 1G

  # =============================================================================
  # KSQLDB CLI
  # =============================================================================

  ksqldb-cli:
    image: confluentinc/cp-ksqldb-cli:7.5.0
    container_name: ksqldb-cli
    depends_on:
      ksqldb-server:
        condition: service_healthy
    entrypoint: /bin/sh
    tty: true
    stdin_open: true
    networks:
      - kafka-network
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M

  # =============================================================================
  # KAFKA UI (for development and monitoring)
  # =============================================================================

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      kafka-1:
        condition: service_healthy
      kafka-2:
        condition: service_healthy
      kafka-3:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
      kafka-connect:
        condition: service_healthy
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: pyairtable-kafka
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-1:29092,kafka-2:29093,kafka-3:29094
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schema-registry:8081
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: pyairtable-connect
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: http://kafka-connect:8083
      KAFKA_CLUSTERS_0_KSQLDBSERVER: http://ksqldb-server:8088
      
      # UI Configuration
      DYNAMIC_CONFIG_ENABLED: 'true'
      AUTH_TYPE: "DISABLED"
      
      # Performance
      KAFKA_CLUSTERS_0_AUDIT_TOPICAUDITENABLED: 'false'
      KAFKA_CLUSTERS_0_AUDIT_CONSOLEAUDITENABLED: 'false'
      
    networks:
      - kafka-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M

  # =============================================================================
  # KAFKA MANAGER
  # =============================================================================

  kafka-manager:
    image: sheepkiller/kafka-manager:latest
    container_name: kafka-manager
    depends_on:
      kafka-1:
        condition: service_healthy
      kafka-2:
        condition: service_healthy
      kafka-3:
        condition: service_healthy
    ports:
      - "9000:9000"
    environment:
      ZK_HOSTS: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      APPLICATION_SECRET: pyairtable-kafka-manager-secret
      KM_ARGS: -Djava.net.preferIPv4Stack=true
    networks:
      - kafka-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M

  # =============================================================================
  # KAFKA TOPIC INITIALIZATION
  # =============================================================================

  # =============================================================================
  # KAFKA EXPORTER FOR PROMETHEUS MONITORING
  # =============================================================================

  kafka-exporter:
    image: danielqsj/kafka-exporter:latest
    container_name: kafka-exporter
    depends_on:
      kafka-1:
        condition: service_healthy
      kafka-2:
        condition: service_healthy
      kafka-3:
        condition: service_healthy
    ports:
      - "9308:9308"
    command:
      - --kafka.server=kafka-1:29092
      - --kafka.server=kafka-2:29095
      - --kafka.server=kafka-3:29098
      - --web.listen-address=0.0.0.0:9308
      - --log.level=info
      - --topic.filter=pyairtable.*
    networks:
      - kafka-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9308/metrics"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M

  # =============================================================================
  # KAFKA BACKUP AND DISASTER RECOVERY
  # =============================================================================

  kafka-mirrormaker:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka-mirrormaker
    depends_on:
      kafka-1:
        condition: service_healthy
      kafka-2:
        condition: service_healthy
      kafka-3:
        condition: service_healthy
    environment:
      KAFKA_HEAP_OPTS: "-Xmx512M -Xms512M"
    volumes:
      - ./kafka-security:/etc/kafka/secrets
      - ./kafka-mirrormaker.properties:/etc/kafka/mirrormaker.properties
    networks:
      - kafka-network
    restart: unless-stopped
    command: >
      bash -c "
        echo 'Starting Kafka MirrorMaker for backup replication...'
        # Note: This would typically mirror to a backup cluster
        # For now, this is a placeholder for disaster recovery setup
        echo 'MirrorMaker configured for disaster recovery'
        sleep infinity
      "
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 768M
        reservations:
          cpus: '0.25'
          memory: 512M

  kafka-init:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka-init
    depends_on:
      kafka-1:
        condition: service_healthy
      kafka-2:
        condition: service_healthy
      kafka-3:
        condition: service_healthy
    networks:
      - kafka-network
    volumes:
      - ./kafka-security:/etc/kafka/secrets
    command: >
      bash -c "
        echo 'Creating Kafka SASL users...'
        
        # Create SASL/SCRAM users (commented out for now - requires admin setup)
        # kafka-configs --bootstrap-server kafka-1:29092 --alter --add-config 'SCRAM-SHA-256=[iterations=8192,password=pyairtable-admin-secret]' --entity-type users --entity-name admin
        # kafka-configs --bootstrap-server kafka-1:29092 --alter --add-config 'SCRAM-SHA-256=[iterations=8192,password=pyairtable-producer-secret]' --entity-type users --entity-name producer
        # kafka-configs --bootstrap-server kafka-1:29092 --alter --add-config 'SCRAM-SHA-256=[iterations=8192,password=pyairtable-consumer-secret]' --entity-type users --entity-name consumer
        
        echo 'Creating Kafka topics...'
        
        # Core Event Topics
        kafka-topics --create --topic pyairtable.auth.events --bootstrap-server kafka-1:29092 --partitions 6 --replication-factor 3 --config retention.ms=604800000 --config compression.type=snappy || true
        kafka-topics --create --topic pyairtable.airtable.events --bootstrap-server kafka-1:29092 --partitions 12 --replication-factor 3 --config retention.ms=1209600000 --config compression.type=snappy || true
        kafka-topics --create --topic pyairtable.files.events --bootstrap-server kafka-1:29092 --partitions 8 --replication-factor 3 --config retention.ms=86400000 --config compression.type=lz4 || true
        kafka-topics --create --topic pyairtable.workflows.events --bootstrap-server kafka-1:29092 --partitions 10 --replication-factor 3 --config retention.ms=2592000000 --config compression.type=snappy || true
        kafka-topics --create --topic pyairtable.ai.events --bootstrap-server kafka-1:29092 --partitions 4 --replication-factor 3 --config retention.ms=604800000 --config compression.type=gzip || true
        kafka-topics --create --topic pyairtable.system.events --bootstrap-server kafka-1:29092 --partitions 6 --replication-factor 3 --config retention.ms=7776000000 --config compression.type=snappy || true
        
        # SAGA and Command Topics
        kafka-topics --create --topic pyairtable.saga.events --bootstrap-server kafka-1:29092 --partitions 6 --replication-factor 3 --config retention.ms=2592000000 --config compression.type=snappy || true
        kafka-topics --create --topic pyairtable.commands --bootstrap-server kafka-1:29092 --partitions 12 --replication-factor 3 --config retention.ms=604800000 --config compression.type=snappy || true
        
        # Analytics and Monitoring Topics
        kafka-topics --create --topic pyairtable.analytics.events --bootstrap-server kafka-1:29092 --partitions 8 --replication-factor 3 --config retention.ms=86400000 --config compression.type=lz4 || true
        kafka-topics --create --topic pyairtable.metrics --bootstrap-server kafka-1:29092 --partitions 4 --replication-factor 3 --config retention.ms=259200000 --config compression.type=snappy || true
        
        # Dead Letter Queue
        kafka-topics --create --topic pyairtable.dlq.events --bootstrap-server kafka-1:29092 --partitions 3 --replication-factor 3 --config retention.ms=2592000000 --config compression.type=snappy || true
        
        # Connect Topics
        kafka-topics --create --topic pyairtable.connect.configs --bootstrap-server kafka-1:29092 --partitions 1 --replication-factor 3 --config cleanup.policy=compact || true
        kafka-topics --create --topic pyairtable.connect.offsets --bootstrap-server kafka-1:29092 --partitions 25 --replication-factor 3 --config cleanup.policy=compact || true
        kafka-topics --create --topic pyairtable.connect.status --bootstrap-server kafka-1:29092 --partitions 5 --replication-factor 3 --config cleanup.policy=compact || true
        
        echo 'Kafka topics created successfully!'
        
        # List all topics
        echo 'Current topics:'
        kafka-topics --list --bootstrap-server kafka-1:29092
      "
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M

networks:
  kafka-network:
    driver: bridge
    name: kafka-network

volumes:
  # ZooKeeper Data
  zookeeper-1-data:
    driver: local
  zookeeper-1-logs:
    driver: local
  zookeeper-2-data:
    driver: local
  zookeeper-2-logs:
    driver: local
  zookeeper-3-data:
    driver: local
  zookeeper-3-logs:
    driver: local
  
  # Kafka Data
  kafka-1-data:
    driver: local
  kafka-2-data:
    driver: local
  kafka-3-data:
    driver: local
  
  # Other Components
  kafka-connect-data:
    driver: local
  ksqldb-data:
    driver: local
  
  # Security and Configuration
  kafka-security:
    driver: local
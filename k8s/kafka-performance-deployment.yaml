apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-performance-config
  namespace: pyairtable
data:
  server.properties: |
    # Optimized Kafka Configuration for High Performance
    broker.id=${KAFKA_BROKER_ID}
    listeners=PLAINTEXT://0.0.0.0:9092,SSL://0.0.0.0:9093
    advertised.listeners=PLAINTEXT://${KAFKA_ADVERTISED_HOST}:9092,SSL://${KAFKA_ADVERTISED_HOST}:9093
    
    # Network and IO Optimization
    num.network.threads=8
    num.io.threads=16
    socket.send.buffer.bytes=102400
    socket.receive.buffer.bytes=102400
    socket.request.max.bytes=104857600
    num.replica.fetchers=4
    
    # Log Configuration for High Throughput
    log.dirs=/kafka-logs
    num.partitions=12
    default.replication.factor=3
    min.insync.replicas=2
    log.retention.hours=168
    log.retention.bytes=1073741824
    log.segment.bytes=1073741824
    log.cleanup.policy=delete
    
    # Performance Tuning
    log.flush.interval.messages=10000
    log.flush.interval.ms=1000
    compression.type=lz4
    
    # Producer Optimization
    batch.size=65536
    linger.ms=10
    buffer.memory=134217728
    max.request.size=1048576
    
    # Consumer Optimization
    fetch.min.bytes=1024
    fetch.max.wait.ms=500
    max.poll.records=500
    session.timeout.ms=30000
    heartbeat.interval.ms=3000
    
    # Topic Management
    auto.create.topics.enable=false
    delete.topic.enable=true
    controlled.shutdown.enable=true
    
    # Replication
    replica.lag.time.max.ms=30000
    replica.socket.timeout.ms=30000
    replica.fetch.max.bytes=1048576
    
    # Security
    security.inter.broker.protocol=PLAINTEXT
    
    # Metrics
    auto.include.jmx.reporter=true
    jmx.port=9999
    
    # Background Processing
    background.threads=10
    log.cleaner.threads=2
    
    # Transaction Support
    transaction.state.log.replication.factor=3
    transaction.state.log.min.isr=2

---
apiVersion: v1
kind: Service
metadata:
  name: kafka-cluster
  namespace: pyairtable
  labels:
    app: kafka
spec:
  ports:
  - port: 9092
    name: plaintext
  - port: 9093
    name: ssl
  - port: 9999
    name: jmx
  clusterIP: None
  selector:
    app: kafka

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka
  namespace: pyairtable
spec:
  serviceName: kafka-cluster
  replicas: 3
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        app: kafka
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - kafka
            topologyKey: kubernetes.io/hostname
      containers:
      - name: kafka
        image: confluentinc/cp-kafka:latest
        ports:
        - containerPort: 9092
          name: plaintext
        - containerPort: 9093
          name: ssl
        - containerPort: 9999
          name: jmx
        env:
        - name: KAFKA_BROKER_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.annotations['kafka.io/broker-id']
        - name: KAFKA_ADVERTISED_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: KAFKA_ZOOKEEPER_CONNECT
          value: "zookeeper:2181"
        - name: KAFKA_OPTS
          value: "-Xms2g -Xmx2g -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:InitiatingHeapOccupancyPercent=35"
        resources:
          requests:
            memory: "3Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        volumeMounts:
        - name: kafka-data
          mountPath: /kafka-logs
        - name: kafka-config
          mountPath: /etc/kafka/server.properties
          subPath: server.properties
        livenessProbe:
          tcpSocket:
            port: 9092
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          tcpSocket:
            port: 9092
          initialDelaySeconds: 20
          periodSeconds: 5
      # JMX Exporter for monitoring
      - name: jmx-exporter
        image: sscaling/jmx-prometheus-exporter:latest
        ports:
        - containerPort: 9308
        env:
        - name: JMX_PORT
          value: "9999"
        - name: HTTP_PORT
          value: "9308"
        - name: CONFIG_YML
          value: |
            rules:
            - pattern: kafka.server<type=(.+), name=(.+)><>Value
              name: kafka_server_$1_$2
            - pattern: kafka.server<type=(.+), name=(.+), clientId=(.+)><>Value
              name: kafka_server_$1_$2
              labels:
                clientId: "$3"
            - pattern: kafka.network<type=(.+), name=(.+)><>Value
              name: kafka_network_$1_$2
            - pattern: kafka.log<type=(.+), name=(.+), topic=(.+), partition=(.+)><>Value
              name: kafka_log_$1_$2
              labels:
                topic: "$3"
                partition: "$4"
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
      volumes:
      - name: kafka-config
        configMap:
          name: kafka-performance-config
  volumeClaimTemplates:
  - metadata:
      name: kafka-data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 100Gi

---
# Zookeeper StatefulSet for Kafka coordination
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: zookeeper
  namespace: pyairtable
spec:
  serviceName: zookeeper
  replicas: 3
  selector:
    matchLabels:
      app: zookeeper
  template:
    metadata:
      labels:
        app: zookeeper
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - zookeeper
            topologyKey: kubernetes.io/hostname
      containers:
      - name: zookeeper
        image: confluentinc/cp-zookeeper:latest
        ports:
        - containerPort: 2181
          name: client
        - containerPort: 2888
          name: peer
        - containerPort: 3888
          name: leader-election
        env:
        - name: ZOOKEEPER_SERVER_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.annotations['zookeeper.io/server-id']
        - name: ZOOKEEPER_CLIENT_PORT
          value: "2181"
        - name: ZOOKEEPER_TICK_TIME
          value: "2000"
        - name: ZOOKEEPER_INIT_LIMIT
          value: "10"
        - name: ZOOKEEPER_SYNC_LIMIT
          value: "5"
        - name: ZOOKEEPER_SERVERS
          value: "zookeeper-0.zookeeper:2888:3888;zookeeper-1.zookeeper:2888:3888;zookeeper-2.zookeeper:2888:3888"
        - name: ZOOKEEPER_MAX_CLIENT_CNXNS
          value: "60"
        - name: ZOOKEEPER_SNAP_RETAIN_COUNT
          value: "3"
        - name: ZOOKEEPER_PURGE_INTERVAL
          value: "12"
        - name: ZOOKEEPER_JVM_FLAGS
          value: "-Xms512m -Xmx512m"
        resources:
          requests:
            memory: "768Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        volumeMounts:
        - name: zookeeper-data
          mountPath: /var/lib/zookeeper/data
        - name: zookeeper-logs
          mountPath: /var/lib/zookeeper/log
        livenessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - "echo ruok | nc localhost 2181 | grep imok"
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - "echo ruok | nc localhost 2181 | grep imok"
          initialDelaySeconds: 20
          periodSeconds: 5
  volumeClaimTemplates:
  - metadata:
      name: zookeeper-data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 20Gi
  - metadata:
      name: zookeeper-logs
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 10Gi

---
apiVersion: v1
kind: Service
metadata:
  name: zookeeper
  namespace: pyairtable
  labels:
    app: zookeeper
spec:
  ports:
  - port: 2181
    name: client
  - port: 2888
    name: peer
  - port: 3888
    name: leader-election
  clusterIP: None
  selector:
    app: zookeeper

---
# Kafka Manager for cluster management
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kafka-manager
  namespace: pyairtable
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka-manager
  template:
    metadata:
      labels:
        app: kafka-manager
    spec:
      containers:
      - name: kafka-manager
        image: hlebalbau/kafka-manager:stable
        ports:
        - containerPort: 9000
        env:
        - name: ZK_HOSTS
          value: "zookeeper:2181"
        - name: APPLICATION_SECRET
          value: "kafka-manager-secret"
        resources:
          requests:
            memory: "512Mi"
            cpu: "200m"
          limits:
            memory: "1Gi"
            cpu: "500m"

---
apiVersion: v1
kind: Service
metadata:
  name: kafka-manager
  namespace: pyairtable
spec:
  selector:
    app: kafka-manager
  ports:
  - port: 9000
    targetPort: 9000
  type: ClusterIP

---
# Kafka Topic Creation Job
apiVersion: batch/v1
kind: Job
metadata:
  name: kafka-topics-setup
  namespace: pyairtable
spec:
  template:
    spec:
      containers:
      - name: kafka-topics
        image: confluentinc/cp-kafka:latest
        command:
        - /bin/bash
        - -c
        - |
          # Wait for Kafka to be ready
          until kafka-topics --bootstrap-server kafka-0.kafka-cluster:9092 --list; do
            echo "Waiting for Kafka to be ready..."
            sleep 5
          done

          # Create topics with optimized configurations
          kafka-topics --create --bootstrap-server kafka-cluster:9092 \
            --topic user-events \
            --partitions 12 \
            --replication-factor 3 \
            --config compression.type=lz4 \
            --config min.insync.replicas=2 \
            --config cleanup.policy=delete \
            --config retention.ms=604800000 \
            --if-not-exists

          kafka-topics --create --bootstrap-server kafka-cluster:9092 \
            --topic workflow-events \
            --partitions 12 \
            --replication-factor 3 \
            --config compression.type=lz4 \
            --config min.insync.replicas=2 \
            --config cleanup.policy=delete \
            --config retention.ms=604800000 \
            --if-not-exists

          kafka-topics --create --bootstrap-server kafka-cluster:9092 \
            --topic analytics-events \
            --partitions 24 \
            --replication-factor 3 \
            --config compression.type=lz4 \
            --config min.insync.replicas=2 \
            --config cleanup.policy=delete \
            --config retention.ms=259200000 \
            --if-not-exists

          kafka-topics --create --bootstrap-server kafka-cluster:9092 \
            --topic audit-events \
            --partitions 6 \
            --replication-factor 3 \
            --config compression.type=lz4 \
            --config min.insync.replicas=2 \
            --config cleanup.policy=compact \
            --config retention.ms=2592000000 \
            --if-not-exists

          kafka-topics --create --bootstrap-server kafka-cluster:9092 \
            --topic dead-letter-queue \
            --partitions 6 \
            --replication-factor 3 \
            --config compression.type=lz4 \
            --config min.insync.replicas=2 \
            --config cleanup.policy=delete \
            --config retention.ms=1209600000 \
            --if-not-exists

          kafka-topics --create --bootstrap-server kafka-cluster:9092 \
            --topic retry-queue \
            --partitions 6 \
            --replication-factor 3 \
            --config compression.type=lz4 \
            --config min.insync.replicas=2 \
            --config cleanup.policy=delete \
            --config retention.ms=86400000 \
            --if-not-exists

          echo "Kafka topics created successfully"
      restartPolicy: OnFailure

---
# HorizontalPodAutoscaler for Kafka consumers
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: kafka-consumer-hpa
  namespace: pyairtable
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: event-consumer-workers
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: kafka_consumer_lag
      target:
        type: AverageValue
        averageValue: "1000"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60

---
# NetworkPolicy for Kafka security
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: kafka-network-policy
  namespace: pyairtable
spec:
  podSelector:
    matchLabels:
      app: kafka
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: pyairtable-services
    - podSelector:
        matchLabels:
          app: kafka-manager
    ports:
    - protocol: TCP
      port: 9092
    - protocol: TCP
      port: 9093
  - from:
    - podSelector:
        matchLabels:
          app: zookeeper
    ports:
    - protocol: TCP
      port: 2181
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: zookeeper
    ports:
    - protocol: TCP
      port: 2181
  - to: []
    ports:
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53
# Predictive Autoscaling for PyAirtable
# ML-based scaling decisions using historical patterns and forecasting

---
# Namespace for predictive scaling components
apiVersion: v1
kind: Namespace
metadata:
  name: predictive-scaling
  labels:
    name: predictive-scaling

---
# ConfigMap with predictive scaling configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: predictive-scaling-config
  namespace: predictive-scaling
data:
  config.yaml: |
    predictive_scaling:
      enabled: true
      forecast_horizon_minutes: 60
      history_window_hours: 168  # 7 days
      confidence_threshold: 0.8
      models:
        - name: "linear_regression"
          weight: 0.3
        - name: "arima"
          weight: 0.4
        - name: "prophet"
          weight: 0.3
      
      services:
        api_gateway:
          prediction_metrics:
            - "http_requests_per_second"
            - "concurrent_connections"
            - "response_time_p95"
          seasonal_patterns:
            - daily: true
            - weekly: true
            - monthly: false
          external_factors:
            - "business_hours"
            - "timezone_distribution"
            - "user_activity_correlation"
          scaling_buffer: 1.2  # 20% buffer above prediction
          
        saga_orchestrator:
          prediction_metrics:
            - "saga_workflow_start_rate"
            - "saga_completion_rate"
            - "kafka_consumer_lag"
          seasonal_patterns:
            - daily: true
            - weekly: true
          external_factors:
            - "batch_job_schedule"
            - "data_sync_patterns"
          scaling_buffer: 1.5  # 50% buffer for bursty workloads
          
        file_service:
          prediction_metrics:
            - "file_upload_rate"
            - "file_processing_queue_depth"
            - "storage_growth_rate"
          seasonal_patterns:
            - daily: true
            - weekly: true
          external_factors:
            - "user_upload_patterns"
            - "business_reporting_cycles"
          scaling_buffer: 2.0  # Higher buffer for file processing spikes
          
        analytics_service:
          prediction_metrics:
            - "analytics_job_submission_rate"
            - "data_processing_volume"
            - "report_generation_requests"
          seasonal_patterns:
            - daily: true
            - weekly: true
            - monthly: true
          external_factors:
            - "end_of_month_reporting"
            - "business_intelligence_usage"
          scaling_buffer: 1.3

---
# ServiceAccount for predictive scaling
apiVersion: v1
kind: ServiceAccount
metadata:
  name: predictive-scaler
  namespace: predictive-scaling
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT_ID:role/pyairtable-predictive-scaler-role

---
# ClusterRole for predictive scaling operations
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: predictive-scaler
rules:
- apiGroups: [""]
  resources: ["pods", "services", "configmaps"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "watch", "patch", "update"]
- apiGroups: ["autoscaling"]
  resources: ["horizontalpodautoscalers"]
  verbs: ["get", "list", "watch", "patch", "update"]
- apiGroups: ["keda.sh"]
  resources: ["scaledobjects", "scaledjobs"]
  verbs: ["get", "list", "watch", "patch", "update"]
- apiGroups: ["metrics.k8s.io"]
  resources: ["pods", "nodes"]
  verbs: ["get", "list"]
- apiGroups: ["custom.metrics.k8s.io"]
  resources: ["*"]
  verbs: ["get", "list"]

---
# ClusterRoleBinding for predictive scaling
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: predictive-scaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: predictive-scaler
subjects:
- kind: ServiceAccount
  name: predictive-scaler
  namespace: predictive-scaling

---
# Predictive Scaling Engine Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: predictive-scaling-engine
  namespace: predictive-scaling
  labels:
    app: predictive-scaling-engine
    app.kubernetes.io/name: predictive-scaling-engine
    app.kubernetes.io/component: ml-predictor
spec:
  replicas: 2
  selector:
    matchLabels:
      app: predictive-scaling-engine
  template:
    metadata:
      labels:
        app: predictive-scaling-engine
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: predictive-scaler
      containers:
      - name: predictive-engine
        image: pyairtable/predictive-scaler:latest
        ports:
        - name: http
          containerPort: 8080
          protocol: TCP
        - name: grpc
          containerPort: 9090
          protocol: TCP
        env:
        - name: PROMETHEUS_URL
          value: "http://prometheus.pyairtable-monitoring.svc.cluster.local:9090"
        - name: REDIS_URL
          valueFrom:
            secretKeyRef:
              name: redis-secret
              key: url
        - name: POSTGRES_URL
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: url
        - name: AWS_REGION
          value: "us-east-1"
        - name: S3_BUCKET_MODELS
          value: "pyairtable-ml-models"
        - name: LOG_LEVEL
          value: "INFO"
        - name: PREDICTION_INTERVAL_SECONDS
          value: "300"  # 5 minutes
        - name: MODEL_UPDATE_INTERVAL_HOURS
          value: "24"   # Daily model updates
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 4Gi
        livenessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 60
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /ready
            port: http
          initialDelaySeconds: 30
          periodSeconds: 10
        volumeMounts:
        - name: config
          mountPath: /etc/config
          readOnly: true
        - name: models
          mountPath: /var/models
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 10001
      volumes:
      - name: config
        configMap:
          name: predictive-scaling-config
      - name: models
        emptyDir: {}

---
# Predictive Scaling Engine Service
apiVersion: v1
kind: Service
metadata:
  name: predictive-scaling-engine
  namespace: predictive-scaling
  labels:
    app: predictive-scaling-engine
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 8080
    targetPort: http
    protocol: TCP
  - name: grpc
    port: 9090
    targetPort: grpc
    protocol: TCP
  selector:
    app: predictive-scaling-engine

---
# CronJob for Historical Data Collection
apiVersion: batch/v1
kind: CronJob
metadata:
  name: historical-data-collector
  namespace: predictive-scaling
spec:
  schedule: "*/15 * * * *"  # Every 15 minutes
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: historical-data-collector
        spec:
          serviceAccountName: predictive-scaler
          restartPolicy: OnFailure
          containers:
          - name: data-collector
            image: pyairtable/data-collector:latest
            env:
            - name: PROMETHEUS_URL
              value: "http://prometheus.pyairtable-monitoring.svc.cluster.local:9090"
            - name: POSTGRES_URL
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: url
            - name: COLLECTION_WINDOW_MINUTES
              value: "15"
            resources:
              requests:
                cpu: 100m
                memory: 256Mi
              limits:
                cpu: 500m
                memory: 512Mi
            command:
            - /bin/sh
            - -c
            - |
              echo "Collecting historical metrics..."
              
              # Collect CPU metrics
              curl -s "${PROMETHEUS_URL}/api/v1/query_range?query=avg(rate(container_cpu_usage_seconds_total[5m]))by(pod)&start=$(date -d '15 minutes ago' -u +%Y-%m-%dT%H:%M:%SZ)&end=$(date -u +%Y-%m-%dT%H:%M:%SZ)&step=60s" > /tmp/cpu_metrics.json
              
              # Collect memory metrics
              curl -s "${PROMETHEUS_URL}/api/v1/query_range?query=avg(container_memory_usage_bytes)by(pod)&start=$(date -d '15 minutes ago' -u +%Y-%m-%dT%H:%M:%SZ)&end=$(date -u +%Y-%m-%dT%H:%M:%SZ)&step=60s" > /tmp/memory_metrics.json
              
              # Collect request rate metrics
              curl -s "${PROMETHEUS_URL}/api/v1/query_range?query=sum(rate(http_requests_total[5m]))by(service)&start=$(date -d '15 minutes ago' -u +%Y-%m-%dT%H:%M:%SZ)&end=$(date -u +%Y-%m-%dT%H:%M:%SZ)&step=60s" > /tmp/request_rate_metrics.json
              
              # Store in database
              python3 -c "
              import json
              import psycopg2
              import os
              from datetime import datetime
              
              conn = psycopg2.connect(os.environ['POSTGRES_URL'])
              cur = conn.cursor()
              
              # Create table if not exists
              cur.execute('''
                CREATE TABLE IF NOT EXISTS predictive_scaling_metrics (
                  id SERIAL PRIMARY KEY,
                  timestamp TIMESTAMP NOT NULL,
                  service_name VARCHAR(100) NOT NULL,
                  metric_name VARCHAR(100) NOT NULL,
                  metric_value FLOAT NOT NULL,
                  metadata JSONB
                );
              ''')
              
              # Insert CPU metrics
              with open('/tmp/cpu_metrics.json') as f:
                data = json.load(f)
                for result in data['data']['result']:
                  pod_name = result['metric']['pod']
                  for timestamp, value in result['values']:
                    cur.execute('''
                      INSERT INTO predictive_scaling_metrics 
                      (timestamp, service_name, metric_name, metric_value, metadata)
                      VALUES (%s, %s, %s, %s, %s)
                    ''', (
                      datetime.fromtimestamp(int(timestamp)),
                      pod_name,
                      'cpu_usage_rate',
                      float(value),
                      json.dumps({'collection_type': 'historical'})
                    ))
              
              conn.commit()
              conn.close()
              print('Historical data collection completed')
              "

---
# CronJob for Model Training
apiVersion: batch/v1
kind: CronJob
metadata:
  name: model-trainer
  namespace: predictive-scaling
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: model-trainer
        spec:
          serviceAccountName: predictive-scaler
          restartPolicy: OnFailure
          containers:
          - name: model-trainer
            image: pyairtable/ml-trainer:latest
            env:
            - name: POSTGRES_URL
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: url
            - name: S3_BUCKET_MODELS
              value: "pyairtable-ml-models"
            - name: AWS_REGION
              value: "us-east-1"
            - name: TRAINING_WINDOW_DAYS
              value: "30"
            resources:
              requests:
                cpu: 1000m
                memory: 2Gi
              limits:
                cpu: 4000m
                memory: 8Gi
            command:
            - python3
            - -c
            - |
              import pandas as pd
              import numpy as np
              from sklearn.ensemble import RandomForestRegressor
              from sklearn.linear_model import LinearRegression
              from sklearn.metrics import mean_absolute_error, mean_squared_error
              import psycopg2
              import boto3
              import pickle
              import json
              import os
              from datetime import datetime, timedelta
              
              print("Starting model training...")
              
              # Connect to database
              conn = psycopg2.connect(os.environ['POSTGRES_URL'])
              
              # Get training data
              query = """
                SELECT timestamp, service_name, metric_name, metric_value
                FROM predictive_scaling_metrics
                WHERE timestamp >= %s
                ORDER BY timestamp ASC
              """
              
              training_start = datetime.now() - timedelta(days=int(os.environ['TRAINING_WINDOW_DAYS']))
              df = pd.read_sql(query, conn, params=[training_start])
              
              # Feature engineering
              df['timestamp'] = pd.to_datetime(df['timestamp'])
              df['hour'] = df['timestamp'].dt.hour
              df['day_of_week'] = df['timestamp'].dt.dayofweek
              df['day_of_month'] = df['timestamp'].dt.day
              
              # Train models for each service and metric combination
              models = {}
              s3_client = boto3.client('s3', region_name=os.environ['AWS_REGION'])
              
              for service in df['service_name'].unique():
                models[service] = {}
                service_data = df[df['service_name'] == service]
                
                for metric in service_data['metric_name'].unique():
                  metric_data = service_data[service_data['metric_name'] == metric].copy()
                  
                  if len(metric_data) < 100:  # Need sufficient data
                    continue
                  
                  # Prepare features
                  features = ['hour', 'day_of_week', 'day_of_month']
                  X = metric_data[features]
                  y = metric_data['metric_value']
                  
                  # Split train/test
                  split_idx = int(len(X) * 0.8)
                  X_train, X_test = X[:split_idx], X[split_idx:]
                  y_train, y_test = y[:split_idx], y[split_idx:]
                  
                  # Train Random Forest model
                  rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
                  rf_model.fit(X_train, y_train)
                  rf_pred = rf_model.predict(X_test)
                  rf_mae = mean_absolute_error(y_test, rf_pred)
                  
                  # Train Linear Regression model
                  lr_model = LinearRegression()
                  lr_model.fit(X_train, y_train)
                  lr_pred = lr_model.predict(X_test)
                  lr_mae = mean_absolute_error(y_test, lr_pred)
                  
                  # Select best model
                  if rf_mae < lr_mae:
                    best_model = rf_model
                    model_type = 'random_forest'
                    best_mae = rf_mae
                  else:
                    best_model = lr_model
                    model_type = 'linear_regression'
                    best_mae = lr_mae
                  
                  models[service][metric] = {
                    'model': best_model,
                    'type': model_type,
                    'mae': best_mae,
                    'trained_at': datetime.now().isoformat()
                  }
                  
                  # Save model to S3
                  model_key = f"models/{service}/{metric}/{model_type}_model.pkl"
                  model_bytes = pickle.dumps(best_model)
                  s3_client.put_object(
                    Bucket=os.environ['S3_BUCKET_MODELS'],
                    Key=model_key,
                    Body=model_bytes
                  )
                  
                  print(f"Trained {model_type} model for {service}.{metric} with MAE: {best_mae:.4f}")
              
              # Save model metadata
              metadata = {
                'trained_at': datetime.now().isoformat(),
                'training_window_days': int(os.environ['TRAINING_WINDOW_DAYS']),
                'models': {
                  service: {
                    metric: {
                      'type': info['type'],
                      'mae': info['mae'],
                      'trained_at': info['trained_at']
                    }
                    for metric, info in service_models.items()
                  }
                  for service, service_models in models.items()
                }
              }
              
              s3_client.put_object(
                Bucket=os.environ['S3_BUCKET_MODELS'],
                Key='metadata/model_metadata.json',
                Body=json.dumps(metadata, indent=2),
                ContentType='application/json'
              )
              
              conn.close()
              print("Model training completed successfully")

---
# Custom Resource for Predictive Scaling Policy
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: predictivescalingpolicies.scaling.pyairtable.com
spec:
  group: scaling.pyairtable.com
  versions:
  - name: v1
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        type: object
        properties:
          spec:
            type: object
            properties:
              targetRef:
                type: object
                properties:
                  apiVersion:
                    type: string
                  kind:
                    type: string
                  name:
                    type: string
              predictionWindow:
                type: string
                description: "How far ahead to predict (e.g., '1h', '30m')"
              confidence:
                type: number
                minimum: 0
                maximum: 1
                description: "Minimum confidence level for predictions"
              scalingBuffer:
                type: number
                minimum: 1
                description: "Buffer factor above predicted need"
              metrics:
                type: array
                items:
                  type: string
          status:
            type: object
            properties:
              lastPrediction:
                type: string
              confidence:
                type: number
              recommendedReplicas:
                type: integer
  scope: Namespaced
  names:
    plural: predictivescalingpolicies
    singular: predictivescalingpolicy
    kind: PredictiveScalingPolicy

---
# Example Predictive Scaling Policy for API Gateway
apiVersion: scaling.pyairtable.com/v1
kind: PredictiveScalingPolicy
metadata:
  name: api-gateway-predictive
  namespace: pyairtable
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-gateway
  predictionWindow: "1h"
  confidence: 0.8
  scalingBuffer: 1.2
  metrics:
  - "http_requests_per_second"
  - "concurrent_connections"
  - "response_time_p95"

---
# ServiceMonitor for Predictive Scaling metrics
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: predictive-scaling-metrics
  namespace: pyairtable-monitoring
  labels:
    app: predictive-scaling-engine
spec:
  selector:
    matchLabels:
      app: predictive-scaling-engine
  namespaceSelector:
    matchNames:
    - predictive-scaling
  endpoints:
  - port: http
    interval: 30s
    path: /metrics

---
# Prometheus Rules for Predictive Scaling
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: predictive-scaling-rules
  namespace: pyairtable-monitoring
  labels:
    app: pyairtable
    component: predictive-scaling
spec:
  groups:
  - name: predictive.scaling.rules
    rules:
    # Prediction accuracy metrics
    - record: predictive_scaling:prediction_accuracy
      expr: |
        1 - (
          abs(predictive_scaling_predicted_value - predictive_scaling_actual_value) /
          predictive_scaling_actual_value
        )
    
    # Model confidence tracking
    - record: predictive_scaling:model_confidence_avg
      expr: avg(predictive_scaling_model_confidence) by (service, model_type)
    
    # Scaling recommendation tracking
    - record: predictive_scaling:scaling_recommendations
      expr: sum(increase(predictive_scaling_recommendations_total[5m])) by (service, action)
    
    # Cost impact of predictive scaling
    - record: predictive_scaling:cost_savings_estimate
      expr: |
        (
          sum(kube_deployment_spec_replicas{deployment=~".*"}) -
          sum(predictive_scaling_recommended_replicas)
        ) * 0.10  # Estimated cost per replica per hour
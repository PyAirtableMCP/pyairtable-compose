version: '3.8'

# Performance Monitoring Stack for PyAirtable
# Enhanced monitoring for 1000 user performance targets
# Includes: Prometheus, Grafana, Jaeger, LGTM stack components

services:
  # Prometheus - Metrics collection with performance focus
  prometheus-performance:
    image: prom/prometheus:v2.47.2
    container_name: prometheus-performance
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=7d'
      - '--storage.tsdb.retention.size=2GB'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
      - '--query.timeout=30s'
      - '--query.max-concurrency=20'
      - '--storage.tsdb.min-block-duration=2h'
      - '--storage.tsdb.max-block-duration=24h'
    volumes:
      - ./monitoring/prometheus/prometheus-performance.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/prometheus/performance-alert-rules.yml:/etc/prometheus/performance-alert-rules.yml:ro
      - prometheus-performance-data:/prometheus
    networks:
      - observability
      - pyairtable-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'

  # Grafana - Performance dashboards
  grafana-performance:
    image: grafana/grafana:10.2.0
    container_name: grafana-performance
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin123}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SECURITY_ALLOW_EMBEDDING=true
      - GF_AUTH_ANONYMOUS_ENABLED=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel,grafana-clock-panel,grafana-polystat-panel,grafana-worldmap-panel
      - GF_FEATURE_TOGGLES_ENABLE=ngalert
      - GF_UNIFIED_ALERTING_ENABLED=true
      - GF_ALERTING_ENABLED=false
      - GF_RENDERING_SERVER_URL=http://grafana-renderer:8081/render
      - GF_RENDERING_CALLBACK_URL=http://grafana-performance:3000/
      - GF_LOG_FILTERS=rendering:debug
    volumes:
      - grafana-performance-data:/var/lib/grafana
      - ./monitoring/grafana/performance-dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/performance-datasources:/etc/grafana/provisioning/datasources:ro
      - ./monitoring/grafana/performance-alerting:/etc/grafana/provisioning/alerting:ro
    networks:
      - observability
    restart: unless-stopped
    depends_on:
      - prometheus-performance
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'

  # Grafana Renderer - For PDF reports and alerts
  grafana-renderer:
    image: grafana/grafana-image-renderer:latest
    container_name: grafana-renderer
    ports:
      - "8081:8081"
    environment:
      - ENABLE_METRICS=true
      - HTTP_PORT=8081
    networks:
      - observability
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.5'

  # Jaeger - Distributed tracing for performance analysis
  jaeger-performance:
    image: jaegertracing/all-in-one:1.50
    container_name: jaeger-performance
    ports:
      - "16686:16686"  # Jaeger UI
      - "14268:14268"  # Jaeger collector HTTP
      - "14250:14250"  # Jaeger collector gRPC
      - "6831:6831/udp"  # Jaeger agent UDP
      - "6832:6832/udp"  # Jaeger agent UDP
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - COLLECTOR_ZIPKIN_HOST_PORT=:9411
      - SPAN_STORAGE_TYPE=badger
      - BADGER_EPHEMERAL=false
      - BADGER_DIRECTORY_VALUE=/badger/data
      - BADGER_DIRECTORY_KEY=/badger/key
      - QUERY_BASE_PATH=/jaeger
      - METRICS_STORAGE_TYPE=prometheus
      - PROMETHEUS_SERVER_URL=http://prometheus-performance:9090
      - PROMETHEUS_QUERY_SUPPORT_SPANMETRICS_CONNECTOR=true
    volumes:
      - jaeger-performance-data:/badger
    networks:
      - observability
      - pyairtable-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'

  # OpenTelemetry Collector - Enhanced for performance monitoring
  otel-collector-performance:
    image: otel/opentelemetry-collector-contrib:0.88.0
    container_name: otel-collector-performance
    command: ["--config=/etc/otel-collector-performance-config.yml"]
    volumes:
      - ./monitoring/otel/otel-collector-performance-config.yml:/etc/otel-collector-performance-config.yml:ro
    ports:
      - "4317:4317"   # OTLP gRPC receiver
      - "4318:4318"   # OTLP HTTP receiver
      - "8888:8888"   # Prometheus metrics
      - "8889:8889"   # Prometheus exporter metrics
      - "13133:13133" # Health check
    networks:
      - observability
      - pyairtable-network
    restart: unless-stopped
    depends_on:
      - jaeger-performance
      - prometheus-performance
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'

  # Loki - Optimized log aggregation
  loki-performance:
    image: grafana/loki:2.9.2
    container_name: loki-performance
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/loki-performance-config.yaml
    volumes:
      - ./monitoring/loki/loki-performance-config.yml:/etc/loki/loki-performance-config.yaml:ro
      - loki-performance-data:/loki
    networks:
      - observability
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 768M
          cpus: '0.5'
        reservations:
          memory: 384M
          cpus: '0.25'

  # Promtail - Enhanced log shipper
  promtail-performance:
    image: grafana/promtail:2.9.2
    container_name: promtail-performance
    command: -config.file=/etc/promtail/promtail-performance-config.yml
    volumes:
      - ./monitoring/promtail/promtail-performance-config.yml:/etc/promtail/promtail-performance-config.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/log:/var/log:ro
    networks:
      - observability
      - pyairtable-network
    restart: unless-stopped
    depends_on:
      - loki-performance
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.2'
        reservations:
          memory: 64M
          cpus: '0.1'

  # AlertManager - Performance alerting
  alertmanager-performance:
    image: prom/alertmanager:v0.26.0
    container_name: alertmanager-performance
    ports:
      - "9093:9093"
    volumes:
      - ./monitoring/alertmanager/alertmanager-performance.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager-performance-data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=http://localhost:9093'
      - '--cluster.listen-address=0.0.0.0:9094'
    networks:
      - observability
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.2'
        reservations:
          memory: 64M
          cpus: '0.1'

  # Node Exporter - System metrics
  node-exporter-performance:
    image: prom/node-exporter:v1.6.1
    container_name: node-exporter-performance
    ports:
      - "9100:9100"
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
      - '--collector.cpu'
      - '--collector.meminfo'
      - '--collector.diskstats'
      - '--collector.filesystem'
      - '--collector.loadavg'
      - '--collector.netdev'
      - '--collector.stat'
      - '--collector.time'
      - '--collector.uname'
      - '--collector.vmstat'
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    networks:
      - observability
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.2'
        reservations:
          memory: 64M
          cpus: '0.1'

  # cAdvisor - Container metrics
  cadvisor-performance:
    image: gcr.io/cadvisor/cadvisor:v0.47.2
    container_name: cadvisor-performance
    ports:
      - "8080:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    privileged: true
    devices:
      - /dev/kmsg
    networks:
      - observability
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.3'
        reservations:
          memory: 128M
          cpus: '0.15'

  # Redis Exporter - Redis performance metrics
  redis-exporter-performance:
    image: oliver006/redis_exporter:v1.55.0
    container_name: redis-exporter-performance
    ports:
      - "9121:9121"
    environment:
      - REDIS_ADDR=redis://redis:6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - REDIS_EXPORTER_CHECK_KEYS=*
      - REDIS_EXPORTER_CHECK_SINGLE_KEYS=*
    networks:
      - observability
      - pyairtable-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 64M
          cpus: '0.1'
        reservations:
          memory: 32M
          cpus: '0.05'

  # Postgres Exporter - Database performance metrics
  postgres-exporter-performance:
    image: prometheuscommunity/postgres-exporter:v0.14.0
    container_name: postgres-exporter-performance
    ports:
      - "9187:9187"
    environment:
      - DATA_SOURCE_NAME=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}?sslmode=disable
      - PG_EXPORTER_EXTEND_QUERY_PATH=/etc/postgres_exporter/queries.yaml
      - PG_EXPORTER_DISABLE_DEFAULT_METRICS=false
    volumes:
      - ./monitoring/postgres/postgres-exporter-queries.yaml:/etc/postgres_exporter/queries.yaml:ro
    networks:
      - observability
      - pyairtable-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 64M
          cpus: '0.1'
        reservations:
          memory: 32M
          cpus: '0.05'

  # Blackbox Exporter - Endpoint monitoring
  blackbox-exporter:
    image: prom/blackbox-exporter:v0.24.0
    container_name: blackbox-exporter
    ports:
      - "9115:9115"
    volumes:
      - ./monitoring/blackbox/blackbox-performance.yml:/etc/blackbox_exporter/config.yml:ro
    networks:
      - observability
      - pyairtable-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 64M
          cpus: '0.1'

  # Performance Testing Dashboard
  performance-dashboard:
    image: nginx:alpine
    container_name: performance-dashboard
    ports:
      - "8090:80"
    volumes:
      - ./monitoring/dashboard:/usr/share/nginx/html:ro
      - ./monitoring/nginx/performance-nginx.conf:/etc/nginx/nginx.conf:ro
    networks:
      - observability
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 64M
          cpus: '0.1'

  # VictoriaMetrics - High-performance metrics storage (alternative to Prometheus)
  victoriametrics:
    image: victoriametrics/victoria-metrics:latest
    container_name: victoriametrics
    ports:
      - "8428:8428"
    volumes:
      - victoriametrics-data:/victoria-metrics-data
    command:
      - '--storageDataPath=/victoria-metrics-data'
      - '--httpListenAddr=:8428'
      - '--retentionPeriod=7d'
      - '--maxLabelsPerTimeseries=30'
      - '--maxConcurrentInserts=16'
      - '--search.maxConcurrentRequests=16'
      - '--search.maxQueryDuration=30s'
      - '--memory.allowedPercent=80'
    networks:
      - observability
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'

networks:
  observability:
    driver: bridge
    name: observability-performance
  pyairtable-network:
    external: true

volumes:
  prometheus-performance-data:
    driver: local
  grafana-performance-data:
    driver: local
  jaeger-performance-data:
    driver: local
  loki-performance-data:
    driver: local
  alertmanager-performance-data:
    driver: local
  victoriametrics-data:
    driver: local
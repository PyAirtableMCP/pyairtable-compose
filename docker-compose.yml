version: '3.8'

services:
  # API Gateway - Main entry point
  api-gateway:
    image: ghcr.io/reg-kris/pyairtable-api-gateway:latest
    build:
      context: ../pyairtable-api-gateway
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - AIRTABLE_GATEWAY_URL=http://airtable-gateway:8002
      - AI_PROCESSING_SERVICE_URL=http://ai-processing-service:8001
      - MCP_SERVER_URL=http://ai-processing-service:8001
      - LLM_ORCHESTRATOR_URL=http://ai-processing-service:8001
      - WORKSPACE_SERVICE_URL=http://workspace-service:8003
      - PLATFORM_SERVICES_URL=http://platform-services:8007
      - AUTOMATION_SERVICES_URL=http://automation-services:8006
      - SAGA_ORCHESTRATOR_URL=http://saga-orchestrator:8008
      - API_KEY=${API_KEY}
      - LOG_LEVEL=${LOG_LEVEL}
      - CORS_ORIGINS=${CORS_ORIGINS}
      - CORS_METHODS=${CORS_METHODS}
      - CORS_HEADERS=${CORS_HEADERS}
      - CORS_CREDENTIALS=${CORS_CREDENTIALS}
      - CORS_MAX_AGE=${CORS_MAX_AGE}
    depends_on:
      airtable-gateway:
        condition: service_healthy
      ai-processing-service:
        condition: service_healthy
      workspace-service:
        condition: service_healthy
      platform-services:
        condition: service_healthy
      automation-services:
        condition: service_healthy
      saga-orchestrator:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - pyairtable-network
    healthcheck:
      test: [
        "CMD-SHELL",
        "python -c \"import urllib.request; urllib.request.urlopen('http://localhost:8000/api/health')\" || exit 1"
      ]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 90s

  # AI Processing Service - Consolidated MCP Server + LLM Orchestrator
  ai-processing-service:
    build:
      context: ../pyairtable-python-services/ai-processing-service
      dockerfile: Dockerfile
    ports:
      - "8001:8001"  # Using MCP server's original port
    environment:
      # Service Configuration
      - API_KEY=${API_KEY}
      - LOG_LEVEL=${LOG_LEVEL}
      - REQUIRE_API_KEY=${REQUIRE_API_KEY:-true}
      # Gemini Configuration (from LLM Orchestrator)
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - THINKING_BUDGET=${THINKING_BUDGET}
      # MCP Configuration (from MCP Server)
      - MCP_SERVER_MODE=http
      # Airtable configuration
      - AIRTABLE_BASE=${AIRTABLE_BASE}
      - AIRTABLE_TOKEN=${AIRTABLE_TOKEN}
      - AIRTABLE_GATEWAY_URL=http://airtable-gateway:8002
      # Redis session storage
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      # Database
      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
    depends_on:
      redis:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - pyairtable-network
    healthcheck:
      test: [
        "CMD-SHELL",
        "curl -f http://localhost:8001/health || exit 1"
      ]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 40s

  # Airtable Gateway - Direct Airtable API integration
  airtable-gateway:
    build:
      context: ../pyairtable-python-services/airtable-gateway
      dockerfile: Dockerfile
    ports:
      - "8002:8002"
    environment:
      - AIRTABLE_TOKEN=${AIRTABLE_TOKEN}
      - AIRTABLE_PAT=${AIRTABLE_TOKEN}
      - AIRTABLE_BASE=${AIRTABLE_BASE}
      - API_KEY=${API_KEY}
      - LOG_LEVEL=${LOG_LEVEL}
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - pyairtable-network
    healthcheck:
      test: [
        "CMD-SHELL",
        "python -c \"import httpx; httpx.get('http://localhost:8002/health').raise_for_status()\" || exit 1"
      ]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 45s

  # Workspace Service - Workspace management and collaboration
  workspace-service:
    build:
      context: ../pyairtable-python-services/workspace-service
      dockerfile: Dockerfile
    ports:
      - "8003:8003"
    environment:
      # Service Configuration
      - ENVIRONMENT=${ENVIRONMENT:-development}
      - LOG_LEVEL=${LOG_LEVEL}
      - API_KEY=${API_KEY}
      - REQUIRE_API_KEY=${REQUIRE_API_KEY:-true}
      # JWT Authentication
      - JWT_SECRET=${JWT_SECRET}
      - JWT_ALGORITHM=${JWT_ALGORITHM:-HS256}
      - JWT_EXPIRES_IN=${JWT_EXPIRES_IN:-24h}
      # Database and Redis
      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379
      - REDIS_STREAMS_URL=redis://:${REDIS_PASSWORD}@redis-streams:6379
      - REDIS_QUEUE_URL=redis://:${REDIS_PASSWORD}@redis-queue:6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      # Event system
      - EVENT_BUS_ENABLED=true
      - PUBLISH_WORKSPACE_EVENTS=true
      # CORS and Security
      - CORS_ORIGINS=${CORS_ORIGINS:-*}
      # Workspace Settings
      - MAX_WORKSPACES_PER_USER=${MAX_WORKSPACES_PER_USER:-50}
      - MAX_MEMBERS_PER_WORKSPACE=${MAX_MEMBERS_PER_WORKSPACE:-100}
      - DEFAULT_WORKSPACE_TEMPLATE=${DEFAULT_WORKSPACE_TEMPLATE:-blank}
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - pyairtable-network
    healthcheck:
      test: [
        "CMD-SHELL",
        "curl -f http://localhost:8003/health || exit 1"
      ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s

  # Phase 4 Consolidated Services
  # Platform Services - Authentication Service (Port 8007) 
  platform-services:
    build:
      context: ../pyairtable-python-services/auth-service
      dockerfile: Dockerfile
    ports:
      - "8007:8007"
    environment:
      # Service Configuration
      - ENVIRONMENT=${ENVIRONMENT:-development}
      - LOG_LEVEL=${LOG_LEVEL}
      - API_KEY=${API_KEY}
      - REQUIRE_API_KEY=${REQUIRE_API_KEY:-true}
      # JWT Authentication
      - JWT_SECRET=${JWT_SECRET}
      - JWT_ALGORITHM=${JWT_ALGORITHM:-HS256}
      - JWT_EXPIRES_IN=${JWT_EXPIRES_IN:-24h}
      # Database and Redis
      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379
      - REDIS_STREAMS_URL=redis://:${REDIS_PASSWORD}@redis-streams:6379
      - REDIS_QUEUE_URL=redis://:${REDIS_PASSWORD}@redis-queue:6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      # Event system
      - EVENT_BUS_ENABLED=true
      - PUBLISH_AUTH_EVENTS=true
      # CORS and Security
      - CORS_ORIGINS=${CORS_ORIGINS:-*}
      # Auth Settings
      - PASSWORD_MIN_LENGTH=${PASSWORD_MIN_LENGTH:-8}
      - PASSWORD_HASH_ROUNDS=${PASSWORD_HASH_ROUNDS:-12}
      # Analytics Settings
      - ANALYTICS_RETENTION_DAYS=${ANALYTICS_RETENTION_DAYS:-90}
      - METRICS_BATCH_SIZE=${METRICS_BATCH_SIZE:-100}
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - pyairtable-network
    healthcheck:
      test: [
        "CMD-SHELL",
        "curl -f http://localhost:8007/health || exit 1"
      ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # Automation Services - Use existing AI service (Port 8006)
  automation-services:
    build:
      context: ../pyairtable-python-services/ai-service
      dockerfile: Dockerfile
    ports:
      - "8006:8006"
    environment:
      - MCP_SERVER_URL=http://ai-processing-service:8001
      - PLATFORM_SERVICES_URL=http://platform-services:8007
      - AUTH_SERVICE_URL=http://platform-services:8007
      - API_KEY=${API_KEY}
      - LOG_LEVEL=${LOG_LEVEL}
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379
      - REDIS_STREAMS_URL=redis://:${REDIS_PASSWORD}@redis-streams:6379
      - REDIS_QUEUE_URL=redis://:${REDIS_PASSWORD}@redis-queue:6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      # Job queue configuration
      - QUEUE_ENABLED=true
      - LONG_RUNNING_JOBS_ENABLED=true
      - MAX_JOB_DURATION=3600
      # File processing configuration
      - MAX_FILE_SIZE=${MAX_FILE_SIZE:-10MB}
      - ALLOWED_EXTENSIONS=${ALLOWED_EXTENSIONS:-pdf,doc,docx,txt,csv,xlsx}
      - UPLOAD_DIR=${UPLOAD_DIR:-/tmp/uploads}
      # Workflow settings
      - DEFAULT_WORKFLOW_TIMEOUT=${DEFAULT_WORKFLOW_TIMEOUT:-300}
      - MAX_WORKFLOW_RETRIES=${MAX_WORKFLOW_RETRIES:-3}
      - SCHEDULER_CHECK_INTERVAL=${SCHEDULER_CHECK_INTERVAL:-30}
    depends_on:
      ai-processing-service:
        condition: service_healthy
      platform-services:
        condition: service_healthy
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - pyairtable-network
    volumes:
      - file-uploads:/tmp/uploads
    healthcheck:
      test: [
        "CMD-SHELL",
        "curl -f http://localhost:8006/health || exit 1"
      ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # SAGA Orchestrator - Distributed transaction coordination (Port 8008)
  saga-orchestrator:
    build:
      context: ../pyairtable-python-services/saga-orchestrator
      dockerfile: Dockerfile
    ports:
      - "8008:8008"
    environment:
      # Service Configuration
      - ENVIRONMENT=${ENVIRONMENT:-development}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - API_KEY=${API_KEY}
      - REQUIRE_API_KEY=${REQUIRE_API_KEY:-false}
      # Database and Redis
      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379
      - REDIS_STREAMS_URL=redis://:${REDIS_PASSWORD}@redis-streams:6379
      - REDIS_QUEUE_URL=redis://:${REDIS_PASSWORD}@redis-queue:6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      # Event Bus Configuration
      - USE_REDIS_EVENT_BUS=true
      - EVENT_STREAM_NAME=saga-events
      - CONSUMER_GROUP=saga-orchestrator
      # SAGA Configuration
      - SAGA_TIMEOUT_SECONDS=${SAGA_TIMEOUT_SECONDS:-3600}
      - SAGA_RETRY_ATTEMPTS=${SAGA_RETRY_ATTEMPTS:-3}
      - SAGA_STEP_TIMEOUT_SECONDS=${SAGA_STEP_TIMEOUT_SECONDS:-300}
      - COMPENSATION_TIMEOUT_SECONDS=${COMPENSATION_TIMEOUT_SECONDS:-600}
      - MAX_CONCURRENT_SAGAS=${MAX_CONCURRENT_SAGAS:-100}
      # Service URLs for SAGA steps
      - AUTH_SERVICE_URL=http://platform-services:8007
      - USER_SERVICE_URL=http://platform-services:8007
      - PERMISSION_SERVICE_URL=http://platform-services:8007
      - NOTIFICATION_SERVICE_URL=http://automation-services:8006
      - AIRTABLE_CONNECTOR_URL=http://airtable-gateway:8002
      - SCHEMA_SERVICE_URL=http://platform-services:8007
      - WEBHOOK_SERVICE_URL=http://automation-services:8006
      - DATA_SYNC_SERVICE_URL=http://automation-services:8006
      # Monitoring
      - ENABLE_METRICS=${ENABLE_METRICS:-true}
      - METRICS_PORT=9090
      # CORS
      - CORS_ORIGINS=${CORS_ORIGINS:-*}
    depends_on:
      redis:
        condition: service_healthy
      # Remove postgres dependency since SAGA orchestrator doesn't need direct DB access
      # postgres:
      #   condition: service_healthy
      # Make other services optional to allow independent startup
      # platform-services:
      #   condition: service_healthy
      # automation-services:
      #   condition: service_healthy
      # airtable-gateway:
      #   condition: service_healthy
    restart: unless-stopped
    networks:
      - pyairtable-network
    healthcheck:
      test: [
        "CMD-SHELL",
        "curl -f http://localhost:8008/health/ || exit 1"
      ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s



  # Redis Master - Event Streams, Pub/Sub, and Job Queues
  redis:
    image: redis:7-alpine
    ports:
      - "6382:6379"  # Expose for event infrastructure access (avoiding port conflict)
    command: >
      redis-server 
      --requirepass ${REDIS_PASSWORD}
      --appendonly yes
      --appendfsync everysec
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --notify-keyspace-events KEA
      --save 900 1
      --save 300 10
      --save 60 10000
    environment:
      - REDIS_PASSWORD=${REDIS_PASSWORD}
    volumes:
      - redis-data:/data
      - redis-config:/usr/local/etc/redis
    restart: unless-stopped
    networks:
      pyairtable-network:
        aliases:
          - redis
          - redis-master
          - event-bus
    healthcheck:
      test: [
        "CMD-SHELL",
        "redis-cli --no-auth-warning -a $$REDIS_PASSWORD ping | grep PONG && redis-cli --no-auth-warning -a $$REDIS_PASSWORD set health_check_key health_check_value && redis-cli --no-auth-warning -a $$REDIS_PASSWORD get health_check_key | grep health_check_value && redis-cli --no-auth-warning -a $$REDIS_PASSWORD del health_check_key"
      ]
      interval: 10s
      timeout: 5s
      retries: 8
      start_period: 15s

  # Redis Streams - Dedicated for event streaming
  redis-streams:
    image: redis:7-alpine
    ports:
      - "6380:6379"
    command: >
      redis-server 
      --requirepass ${REDIS_PASSWORD}
      --appendonly yes
      --appendfsync everysec
      --maxmemory 1gb
      --maxmemory-policy noeviction
      --notify-keyspace-events KEA
    environment:
      - REDIS_PASSWORD=${REDIS_PASSWORD}
    volumes:
      - redis-streams-data:/data
    restart: unless-stopped
    networks:
      pyairtable-network:
        aliases:
          - redis-streams
          - event-stream
    healthcheck:
      test: [
        "CMD-SHELL",
        "redis-cli --no-auth-warning -a $$REDIS_PASSWORD ping | grep PONG"
      ]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 10s

  # Redis Queue - Job queue processing with persistence
  redis-queue:
    image: redis:7-alpine
    ports:
      - "6381:6379"
    command: >
      redis-server 
      --requirepass ${REDIS_PASSWORD}
      --appendonly yes
      --appendfsync always
      --maxmemory 512mb
      --maxmemory-policy noeviction
      --save 60 1000
    environment:
      - REDIS_PASSWORD=${REDIS_PASSWORD}
    volumes:
      - redis-queue-data:/data
    restart: unless-stopped
    networks:
      pyairtable-network:
        aliases:
          - redis-queue
          - job-queue
    healthcheck:
      test: [
        "CMD-SHELL",
        "redis-cli --no-auth-warning -a $$REDIS_PASSWORD ping | grep PONG"
      ]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 10s

  # PostgreSQL - Optimized database for sessions and metadata
  postgres:
    image: postgres:16-alpine
    # SECURITY: Remove exposed port - internal access only
    # ports:
    #   - "5432:5432"
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_INITDB_ARGS=--auth-host=md5
      # Performance optimization environment variables
      - POSTGRES_SHARED_PRELOAD_LIBRARIES=pg_stat_statements,auto_explain
      - POSTGRES_MAX_CONNECTIONS=100
      - POSTGRES_SHARED_BUFFERS=256MB
      - POSTGRES_EFFECTIVE_CACHE_SIZE=1GB
      - POSTGRES_WORK_MEM=4MB
      - POSTGRES_MAINTENANCE_WORK_MEM=64MB
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./init-db.sql:/docker-entrypoint-initdb.d/init-db.sql:ro
      - ./migrations:/docker-entrypoint-initdb.d/migrations:ro
      - ./postgres-optimization.conf:/etc/postgresql/postgresql.conf:ro
      - ./migrations/run_migrations.sh:/docker-entrypoint-initdb.d/999_run_migrations.sh:ro
    command: [
      "postgres",
      "-c", "config_file=/etc/postgresql/postgresql.conf",
      "-c", "shared_preload_libraries=pg_stat_statements,auto_explain",
      "-c", "log_statement=all",
      "-c", "log_min_duration_statement=250",
      "-c", "pg_stat_statements.track=all"
    ]
    restart: unless-stopped
    networks:
      pyairtable-network:
        aliases:
          - postgres
          - db
    healthcheck:
      test: [
        "CMD-SHELL",
        "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB -h localhost -p 5432 && PGPASSWORD=$$POSTGRES_PASSWORD psql -U $$POSTGRES_USER -d $$POSTGRES_DB -c 'SELECT 1;' >/dev/null 2>&1"
      ]
      interval: 15s
      timeout: 10s
      retries: 8
      start_period: 60s

  # BullMQ Queue System - Job processing with persistence
  queue-processor:
    image: node:18-alpine
    working_dir: /app
    ports:
      - "8009:3000"  # Queue UI port as specified
    environment:
      - NODE_ENV=production
      - REDIS_HOST=redis-queue
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - REDIS_DB=0
      - QUEUE_UI_PORT=3000
      - QUEUE_UI_HOST=0.0.0.0
      # Job processing settings
      - MAX_JOB_ATTEMPTS=3
      - JOB_TIMEOUT=3600000  # 60 minutes in milliseconds
      - CONCURRENCY=5
      - FAILED_JOB_TTL=86400  # 24 hours
      - COMPLETED_JOB_TTL=3600  # 1 hour
    volumes:
      - ./queue-system:/app
    command: >
      sh -c "
      apk add --no-cache curl &&
      npm install @bull-board/api@latest @bull-board/express@latest bullmq@latest express@latest redis@latest &&
      node server.js
      "
    depends_on:
      redis-queue:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - pyairtable-network
    healthcheck:
      test: [
        "CMD-SHELL",
        "curl -f http://localhost:3000/health || exit 1"
      ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # Event Stream Processor - Real-time event processing
  event-processor:
    image: node:18-alpine
    working_dir: /app
    environment:
      - NODE_ENV=production
      - REDIS_HOST=redis-streams
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - STREAM_CONSUMER_GROUP=event-processors
      - STREAM_CONSUMER_NAME=processor-1
      - BATCH_SIZE=10
      - BLOCK_TIME=5000
    volumes:
      - ./event-system:/app
    command: >
      sh -c "
      npm install redis@latest &&
      node event-processor.js
      "
    depends_on:
      redis-streams:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - pyairtable-network
    healthcheck:
      test: [
        "CMD-SHELL",
        "ps aux | grep 'node event-processor.js' | grep -v grep || exit 1"
      ]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s

  # Dead Letter Queue Processor - Handle failed events
  dlq-processor:
    image: node:18-alpine
    working_dir: /app
    environment:
      - NODE_ENV=production
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - DLQ_QUEUE_NAME=dead-letter-queue
      - RETRY_ATTEMPTS=3
      - RETRY_DELAY=60000  # 1 minute
    volumes:
      - ./dlq-system:/app
    command: >
      sh -c "
      npm install bullmq@latest redis@latest &&
      node dlq-processor.js
      "
    depends_on:
      redis:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - pyairtable-network
    healthcheck:
      test: [
        "CMD-SHELL",
        "ps aux | grep 'node dlq-processor.js' | grep -v grep || exit 1"
      ]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s

  # Next.js Frontend - Web interface
  frontend:
    image: ghcr.io/reg-kris/pyairtable-tenant-dashboard:latest
    build:
      context: ./frontend-services/tenant-dashboard
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    environment:
      # API Gateway endpoint
      - NEXT_PUBLIC_API_URL=http://localhost:8000
      - NEXT_PUBLIC_API_GATEWAY_URL=http://api-gateway:8000
      # Internal service URLs (for SSR/API routes)
      - LLM_ORCHESTRATOR_URL=http://ai-processing-service:8001
      - MCP_SERVER_URL=http://ai-processing-service:8001
      - AIRTABLE_GATEWAY_URL=http://airtable-gateway:8002
      - PLATFORM_SERVICES_URL=http://platform-services:8007
      - AUTOMATION_SERVICES_URL=http://automation-services:8006
      - SAGA_ORCHESTRATOR_URL=http://saga-orchestrator:8008
      # Authentication and security
      - API_KEY=${API_KEY}
      - NEXTAUTH_SECRET=${NEXTAUTH_SECRET}
      - NEXTAUTH_URL=http://localhost:3000
      # Development configuration
      - NODE_ENV=${NODE_ENV:-development}
      - LOG_LEVEL=${LOG_LEVEL:-info}
      # Feature flags
      - NEXT_PUBLIC_ENABLE_DEBUG=${ENABLE_DEBUG:-false}
      - NEXT_PUBLIC_SHOW_COST_TRACKING=${SHOW_COST_TRACKING:-true}
      # WebSocket configuration for real-time updates
      - NEXT_PUBLIC_WS_URL=ws://localhost:8000/ws
      # Database URL for API routes (not exposed externally)
      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
    volumes:
      # Enable hot reloading in development
      - ./frontend-services/tenant-dashboard:/app
      - /app/node_modules
      - /app/.next
    depends_on:
      api-gateway:
        condition: service_healthy
      ai-processing-service:
        condition: service_healthy
      airtable-gateway:
        condition: service_healthy
      platform-services:
        condition: service_healthy
      automation-services:
        condition: service_healthy
      saga-orchestrator:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - pyairtable-network
    healthcheck:
      test: [
        "CMD-SHELL",
        "curl -f http://localhost:3000/api/health && curl -f http://localhost:3000/health/ready || exit 1"
      ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

networks:
  pyairtable-network:
    driver: bridge

volumes:
  redis-data:
    driver: local
  redis-streams-data:
    driver: local
  redis-queue-data:
    driver: local
  redis-config:
    driver: local
  postgres-data:
    driver: local
  file-uploads:
    driver: local
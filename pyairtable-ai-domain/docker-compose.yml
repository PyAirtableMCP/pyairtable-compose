version: '3.8'

services:
  ai-domain:
    build:
      context: .
      target: development
    ports:
      - "8080:8080"
      - "5678:5678"  # Debug port
    environment:
      - PORT=8080
      - HOST=0.0.0.0
      - DEBUG=true
      - LOG_LEVEL=DEBUG
      - DATABASE_URL=postgresql://postgres:password@postgres:5432/pyairtable_ai
      - REDIS_URL=redis://redis:6379/0
      # LLM Provider Keys (set these in .env)
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - GEMINI_API_KEY=${GEMINI_API_KEY:-}
      # Vector Database Settings
      - VECTOR_DB_PROVIDER=qdrant
      - QDRANT_URL=http://qdrant:6333
      # Service URLs
      - AIRTABLE_GATEWAY_URL=http://airtable-gateway:8001
      - AUTH_SERVICE_URL=http://auth-service:8002
    volumes:
      - ./src:/app/src
      - ./configs:/app/configs
      - ./tests:/app/tests
      - ai-models:/app/models  # Persistent model storage
    depends_on:
      - postgres
      - redis
      - qdrant
    networks:
      - ai-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  postgres:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=pyairtable_ai
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    ports:
      - "5433:5432"  # Different port to avoid conflicts
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./migrations:/docker-entrypoint-initdb.d
    networks:
      - ai-network
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    ports:
      - "6380:6379"  # Different port to avoid conflicts
    volumes:
      - redis-data:/data
    networks:
      - ai-network
    restart: unless-stopped
    command: redis-server --appendonly yes

  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant-data:/qdrant/storage
    networks:
      - ai-network
    restart: unless-stopped
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334

  # Optional: Local Ollama for offline LLM inference
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - ai-network
    restart: unless-stopped
    # Uncomment the following lines if you have GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # Optional: Prometheus for metrics
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./configs/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    networks:
      - ai-network
    restart: unless-stopped
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'

volumes:
  postgres-data:
  redis-data:
  qdrant-data:
  ollama-data:
  prometheus-data:
  ai-models:  # For storing downloaded AI models

networks:
  ai-network:
    driver: bridge
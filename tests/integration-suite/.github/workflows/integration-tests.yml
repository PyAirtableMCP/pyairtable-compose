name: PyAirtable Integration Test Suite

on:
  push:
    branches: [ main, develop, 'feature/*', 'release/*' ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_category:
        description: 'Test category to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - smoke
          - e2e
          - service-integration
          - performance
          - chaos
          - contract
          - security
      environment:
        description: 'Test environment'
        required: false
        default: 'ci'
        type: choice
        options:
          - ci
          - staging
          - production-like
      parallel_execution:
        description: 'Enable parallel test execution'
        required: false
        default: true
        type: boolean

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  # Pre-flight checks and setup
  preflight:
    name: Pre-flight Checks
    runs-on: ubuntu-latest
    outputs:
      test-matrix: ${{ steps.test-matrix.outputs.matrix }}
      should-run-tests: ${{ steps.changes.outputs.should-run }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Check for relevant changes
        id: changes
        uses: dorny/paths-filter@v2
        with:
          filters: |
            should-run:
              - 'go-services/**'
              - 'python-services/**'
              - 'tests/**'
              - 'docker-compose*.yml'
              - 'infrastructure/**'
              - '.github/workflows/**'

      - name: Generate test matrix
        id: test-matrix
        run: |
          if [[ "${{ github.event.inputs.test_category }}" == "all" || -z "${{ github.event.inputs.test_category }}" ]]; then
            if [[ "${{ github.event_name }}" == "pull_request" ]]; then
              # Lighter test suite for PRs
              matrix='["smoke", "e2e", "service-integration"]'
            else
              # Full test suite for main branch and scheduled runs
              matrix='["smoke", "e2e", "service-integration", "performance", "contract"]'
            fi
          else
            matrix='["${{ github.event.inputs.test_category }}"]'
          fi
          
          echo "matrix=$matrix" >> $GITHUB_OUTPUT

      - name: Validate configuration
        run: |
          cd tests/integration-suite
          python3 -c "import yaml; yaml.safe_load(open('config/test-config.yml'))"

  # Build and push test images
  build-images:
    name: Build Test Images
    runs-on: ubuntu-latest
    needs: preflight
    if: needs.preflight.outputs.should-run-tests == 'true'
    outputs:
      image-digest: ${{ steps.build.outputs.digest }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/test-runner
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix=commit-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push test runner image
        id: build
        uses: docker/build-push-action@v5
        with:
          context: tests/integration-suite
          file: tests/integration-suite/Dockerfile.test
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64

  # Matrix test execution
  integration-tests:
    name: Integration Tests - ${{ matrix.category }}
    runs-on: ubuntu-latest
    needs: [preflight, build-images]
    if: needs.preflight.outputs.should-run-tests == 'true'
    strategy:
      fail-fast: false
      matrix:
        category: ${{ fromJson(needs.preflight.outputs.test-matrix) }}
    timeout-minutes: 60
    env:
      TEST_CATEGORY: ${{ matrix.category }}
      TEST_ENVIRONMENT: ${{ github.event.inputs.environment || 'ci' }}
      PARALLEL_EXECUTION: ${{ github.event.inputs.parallel_execution || 'true' }}
      DOCKER_BUILDKIT: 1
      COMPOSE_DOCKER_CLI_BUILD: 1
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Create test environment
        run: |
          cd tests/integration-suite
          cp config/test-config.yml config/test-config-ci.yml
          
          # Adjust configuration for CI environment
          sed -i 's/parallel_execution: true/parallel_execution: ${{ env.PARALLEL_EXECUTION }}/' config/test-config-ci.yml
          sed -i 's/cleanup_strategy: "after_each"/cleanup_strategy: "aggressive"/' config/test-config-ci.yml

      - name: Start test infrastructure
        run: |
          cd tests/integration-suite
          # Start infrastructure services first
          docker-compose -f docker-compose.test.yml up -d \
            postgres-primary \
            postgres-events \
            redis-cache \
            redis-sessions \
            zookeeper \
            kafka
          
          # Wait for infrastructure to be ready
          ./scripts/wait-for-infrastructure.sh

      - name: Start application services
        run: |
          cd tests/integration-suite
          docker-compose -f docker-compose.test.yml up -d \
            auth-service \
            user-service \
            workspace-service \
            permission-service \
            saga-orchestrator \
            api-gateway
          
          # Wait for services to be healthy
          ./scripts/wait-for-services.sh

      - name: Run health checks
        run: |
          cd tests/integration-suite
          ./run-integration-suite.sh --health-check --env ${{ env.TEST_ENVIRONMENT }}

      - name: Execute integration tests
        id: test-execution
        run: |
          cd tests/integration-suite
          
          # Set test parameters based on category
          case "${{ matrix.category }}" in
            "smoke")
              timeout="5m"
              ;;
            "e2e")
              timeout="30m"
              ;;
            "performance")
              timeout="45m"
              ;;
            "chaos")
              timeout="40m"
              ;;
            *)
              timeout="20m"
              ;;
          esac
          
          # Run tests with appropriate timeout
          ./run-integration-suite.sh \
            --category ${{ matrix.category }} \
            --env ${{ env.TEST_ENVIRONMENT }} \
            --timeout $timeout \
            --output json \
            --reports-dir test-results-${{ matrix.category }} \
            --no-cleanup
        continue-on-error: true

      - name: Collect test results
        if: always()
        run: |
          cd tests/integration-suite
          
          # Create results directory
          mkdir -p test-results-${{ matrix.category }}/artifacts
          
          # Collect service logs
          docker-compose -f docker-compose.test.yml logs > test-results-${{ matrix.category }}/artifacts/service-logs.txt
          
          # Collect container stats
          docker stats --no-stream > test-results-${{ matrix.category }}/artifacts/container-stats.txt
          
          # Collect system information
          df -h > test-results-${{ matrix.category }}/artifacts/disk-usage.txt
          free -h > test-results-${{ matrix.category }}/artifacts/memory-usage.txt
          
          # Generate test summary
          echo "Test Category: ${{ matrix.category }}" > test-results-${{ matrix.category }}/summary.txt
          echo "Environment: ${{ env.TEST_ENVIRONMENT }}" >> test-results-${{ matrix.category }}/summary.txt
          echo "Commit: ${{ github.sha }}" >> test-results-${{ matrix.category }}/summary.txt
          echo "Branch: ${{ github.ref_name }}" >> test-results-${{ matrix.category }}/summary.txt
          echo "Timestamp: $(date -u)" >> test-results-${{ matrix.category }}/summary.txt

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.category }}-${{ github.run_number }}
          path: tests/integration-suite/test-results-${{ matrix.category }}/
          retention-days: 30

      - name: Upload coverage reports
        if: always() && matrix.category == 'e2e'
        uses: codecov/codecov-action@v3
        with:
          files: tests/integration-suite/test-results-${{ matrix.category }}/coverage.xml
          flags: integration-tests
          name: integration-coverage-${{ matrix.category }}

      - name: Cleanup test environment
        if: always()
        run: |
          cd tests/integration-suite
          docker-compose -f docker-compose.test.yml down -v --remove-orphans
          docker system prune -f

  # Security scanning
  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    needs: [preflight, build-images]
    if: needs.preflight.outputs.should-run-tests == 'true'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'

      - name: Upload Trivy scan results
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'

  # Performance baseline validation
  performance-baseline:
    name: Performance Baseline
    runs-on: ubuntu-latest
    needs: [preflight, build-images]
    if: needs.preflight.outputs.should-run-tests == 'true' && github.event_name != 'pull_request'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up test environment
        run: |
          cd tests/integration-suite
          ./run-integration-suite.sh --category performance --env ci --no-cleanup

      - name: Store performance baseline
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: 'customSmallerIsBetter'
          output-file-path: tests/integration-suite/test-results/performance-metrics.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          comment-on-alert: true
          alert-threshold: '150%'
          fail-on-alert: false

  # Test result aggregation and reporting
  test-results:
    name: Aggregate Test Results
    runs-on: ubuntu-latest
    needs: [integration-tests, security-scan]
    if: always() && needs.preflight.outputs.should-run-tests == 'true'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download all test results
        uses: actions/download-artifact@v4
        with:
          path: test-results/

      - name: Generate combined report
        run: |
          mkdir -p combined-results
          
          # Combine all test results
          python3 .github/scripts/combine-test-results.py \
            --input-dir test-results/ \
            --output-dir combined-results/ \
            --format html,json,junit
          
          # Generate metrics summary
          python3 .github/scripts/generate-metrics.py \
            --input-dir test-results/ \
            --output combined-results/metrics.json

      - name: Upload combined results
        uses: actions/upload-artifact@v4
        with:
          name: combined-test-results-${{ github.run_number }}
          path: combined-results/
          retention-days: 90

      - name: Publish test results
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: Integration Test Results
          path: 'combined-results/junit.xml'
          reporter: java-junit
          fail-on-error: false

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            try {
              const metrics = JSON.parse(fs.readFileSync('combined-results/metrics.json', 'utf8'));
              
              const comment = `## üß™ Integration Test Results
              
              | Category | Status | Tests | Duration |
              |----------|--------|-------|----------|
              ${metrics.categories.map(cat => 
                `| ${cat.name} | ${cat.status === 'passed' ? '‚úÖ' : '‚ùå'} | ${cat.total} | ${cat.duration} |`
              ).join('\n')}
              
              **Overall Status**: ${metrics.overall_status === 'passed' ? '‚úÖ PASSED' : '‚ùå FAILED'}
              
              üìä [View detailed results](${context.payload.pull_request.html_url}/checks)
              `;
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            } catch (error) {
              console.log('Error posting comment:', error);
            }

      - name: Set job status
        if: always()
        run: |
          if [[ -f "combined-results/metrics.json" ]]; then
            status=$(jq -r '.overall_status' combined-results/metrics.json)
            if [[ "$status" != "passed" ]]; then
              echo "Integration tests failed"
              exit 1
            fi
          else
            echo "No test results found"
            exit 1
          fi

  # Deployment to staging (if tests pass)
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [test-results]
    if: success() && github.ref == 'refs/heads/main' && github.event_name != 'pull_request'
    environment: staging
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Deploy to staging environment
        run: |
          echo "Deploying to staging environment..."
          # Add your deployment logic here
          # This could trigger a deployment pipeline or update staging infrastructure

      - name: Run staging smoke tests
        run: |
          cd tests/integration-suite
          ./run-integration-suite.sh \
            --category smoke \
            --env staging \
            --timeout 10m

  # Notification
  notify:
    name: Notify Results
    runs-on: ubuntu-latest
    needs: [test-results, deploy-staging]
    if: always() && (failure() || success())
    steps:
      - name: Notify Slack on failure
        if: failure()
        uses: 8398a7/action-slack@v3
        with:
          status: failure
          channel: '#engineering'
          text: |
            üö® Integration tests failed for ${{ github.repository }}
            Branch: ${{ github.ref_name }}
            Commit: ${{ github.sha }}
            Workflow: ${{ github.workflow }}
            
            View details: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

      - name: Notify Slack on success
        if: success() && github.ref == 'refs/heads/main'
        uses: 8398a7/action-slack@v3
        with:
          status: success
          channel: '#engineering'
          text: |
            ‚úÖ Integration tests passed for ${{ github.repository }}
            Branch: ${{ github.ref_name }}
            Deployed to staging: ${{ needs.deploy-staging.result == 'success' }}
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

# Jobs for scheduled runs (additional monitoring)
  scheduled-monitoring:
    name: Scheduled Monitoring
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    needs: [test-results]
    steps:
      - name: Check system health trends
        run: |
          # Analyze test results over time
          # Generate health metrics reports
          # Update monitoring dashboards
          echo "Analyzing system health trends..."

      - name: Generate weekly report
        if: github.event.schedule == '0 2 * * 1' # Monday 2 AM
        run: |
          # Generate weekly test results summary
          # Send to stakeholders
          echo "Generating weekly integration test report..."